{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T13:06:00.376002Z",
     "start_time": "2021-05-16T13:05:58.243575Z"
    }
   },
   "outputs": [],
=======
   "id": "71d232d0",
   "metadata": {},
   "outputs": [],
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
<<<<<<< HEAD
=======
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
<<<<<<< HEAD
=======
    "from hyperopt.pyll.base import scope\n",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from kaggler.model import AutoLGB\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, log_loss\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "008bbc4c",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "# 1. 문제 정의"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "75199061",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "# 2. 데이터 수집"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "3ac3e046",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "## (1) 데이콘 기본 데이터"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T13:06:04.656025Z",
     "start_time": "2021-05-16T13:06:04.559221Z"
    }
   },
=======
   "execution_count": null,
   "id": "0fb3d621",
   "metadata": {},
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv').drop(['index'], axis=1).fillna('NAN')\n",
    "test = pd.read_csv('data/test.csv').drop(['index'], axis=1).fillna('NAN')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "7a43d960",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "# 3. 탐색적 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "399e72c1",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "# 4. 변수 조정"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T13:06:13.262158Z",
     "start_time": "2021-05-16T13:06:13.060162Z"
    }
   },
=======
   "execution_count": null,
   "id": "87cf3e79",
   "metadata": {},
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "outputs": [],
   "source": [
    "# train데이터와 test데이터 변수를 함께 조정하기 위해 병합\n",
    "merge_data = pd.concat([train, test], axis = 0)\n",
    "\n",
    "# DAYS_BIRTH\n",
    "merge_data['DAYS_BIRTH_month']=np.floor((-merge_data['DAYS_BIRTH'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_BIRTH_week']=np.floor((-merge_data['DAYS_BIRTH'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_EMPLOYED\n",
    "merge_data['DAYS_EMPLOYED_month']=np.floor((-merge_data['DAYS_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_EMPLOYED_week']=np.floor((-merge_data['DAYS_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# before_EMPLOYED\n",
    "merge_data['before_EMPLOYED']=merge_data['DAYS_BIRTH']-merge_data['DAYS_EMPLOYED']\n",
    "merge_data['before_EMPLOYED_month']=np.floor((-merge_data['before_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['before_EMPLOYED_week']=np.floor((-merge_data['before_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_BIRTH\n",
    "merge_data['1new_1'] = merge_data['DAYS_BIRTH_month'] / merge_data['income_total']\n",
    "merge_data['2new_1'] = merge_data['DAYS_BIRTH_week'] / merge_data['income_total']\n",
    "\n",
    "# DAYS_EMPLOYED\n",
    "merge_data['10new_1'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['11new_1'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# before_EMPLOYED\n",
    "merge_data['12new_1'] = merge_data['before_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['13new_1'] = merge_data['before_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['14new_1'] = merge_data['before_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# 총 수익을 가족 수로 나누기\n",
    "merge_data['15new_1'] = merge_data['income_total'] / merge_data['family_size']\n",
    "\n",
    "# 융합 삭제\n",
    "#merge_data['3new_1'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['DAYS_BIRTH_month']\n",
    "#merge_data['4new_1'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['DAYS_BIRTH_week']\n",
    "#merge_data['5new_1'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['DAYS_BIRTH_month']\n",
    "#merge_data['6new_1'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['DAYS_BIRTH_week']\n",
    "\n",
    "#merge_data['7new_1'] =  merge_data['begin_month'] / merge_data['DAYS_BIRTH_month']\n",
    "#merge_data['8new_1'] =  merge_data['begin_month'] / merge_data['DAYS_EMPLOYED_month']\n",
    "#merge_data['9new_1'] =  merge_data['begin_month'] / merge_data['before_EMPLOYED_month']\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "# 소득대비 \n",
    "merge_data['DAYS_BIRTH'] = merge_data['DAYS_BIRTH'] / -365\n",
    "merge_data['DAYS_EMPLOYED'] = merge_data['DAYS_EMPLOYED'] / -365\n",
    "\n",
=======
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "merge_data['new_1'] = merge_data['child_num'] / merge_data['income_total']\n",
    "merge_data['new_2'] = merge_data['family_size'] / merge_data['income_total']\n",
    "merge_data['new_3'] = merge_data['DAYS_BIRTH'] / merge_data['income_total']\n",
    "merge_data['new_4'] = merge_data['DAYS_EMPLOYED'] / merge_data['income_total']\n",
    "#merge_data['new_5'] = merge_data['begin_month'] / merge_data['income_total']\n",
    "merge_data['new_6'] =  merge_data['DAYS_EMPLOYED'] / merge_data['DAYS_BIRTH']\n",
    "\n",
    "# 소득 skewed-data 처리\n",
<<<<<<< HEAD
    "#merge_data['log1p_income_total'] = np.log1p(merge_data['income_total'])\n",
=======
    "merge_data['log1p_income_total'] = np.log1p(merge_data['income_total'])\n",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "#merge_data['log_income_total'] = np.log(merge_data['income_total'])\n",
    "#merge_data['sqrt_income_total'] = np.sqrt(merge_data['income_total'])\n",
    "#merge_data['boxcox_income_total'] = stats.boxcox(merge_data['income_total'])[0]\n",
    "\n",
    "merge_data = merge_data.fillna(-999)\n",
    "train = merge_data[merge_data['credit'] != -999]\n",
    "test = merge_data[merge_data['credit'] == -999]\n",
    "test.drop('credit', axis = 1, inplace = True)\n",
    "\n",
    "train_cols = list(train.columns); train_cols.remove('credit'); train_cols.append('credit')\n",
    "train = train[train_cols]"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "4b3cd18a",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {
    "scrolled": false
   },
   "source": [
    "train = train[train['child_num']<=6].reset_index(drop=True) # 아이의 수가 7명 이상인 데이터 제거"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "59b7bedc",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "## 인코딩"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T13:06:15.401883Z",
     "start_time": "2021-05-16T13:06:15.388892Z"
    }
   },
   "outputs": [],
   "source": [
    "train_oh = train.copy()\n",
    "train_noh = train.copy()\n",
=======
   "execution_count": null,
   "id": "32bbe515",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oh = train.copy() # oh : 원핫 (->xgb & random forest) \n",
    "train_noh = train.copy() # noh : 범주형으로만 바꿈 Non 원핫(-> lgb)\n",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "test_oh = test.copy()\n",
    "test_noh = test.copy()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T13:06:16.206845Z",
     "start_time": "2021-05-16T13:06:16.175957Z"
    }
   },
=======
   "execution_count": null,
   "id": "21f70cc1",
   "metadata": {},
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "outputs": [],
   "source": [
    "object_col = []\n",
    "for col in train_noh.columns:\n",
    "    if train_noh[col].dtype == 'object':\n",
    "        train_noh[col] = train_noh[col].astype('category')\n",
    "        test_noh[col] = test_noh[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T13:06:17.884151Z",
     "start_time": "2021-05-16T13:06:17.804365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gender', 'car', 'reality', 'income_type', 'edu_type', 'family_type', 'house_type', 'occyp_type']\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "954998c9",
   "metadata": {},
   "outputs": [],
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "source": [
    "object_col = []\n",
    "for col in train_oh.columns:\n",
    "    if train_oh[col].dtype == 'object':\n",
    "        object_col.append(col)\n",
    "print(object_col)        \n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train_oh.drop(object_col, axis=1, inplace=True)\n",
    "train_oh = pd.concat([train_oh, train_onehot_df], axis=1)    \n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test_oh.drop(object_col, axis=1, inplace=True)\n",
    "test_oh = pd.concat([test_oh, test_onehot_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "metadata": {},
   "source": [
    "## 변수 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T13:06:18.800833Z",
     "start_time": "2021-05-16T13:06:18.791858Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x = train_noh.drop(['credit'], axis=1)\n",
    "train_y = train_noh['credit']\n",
    "test_x = test_noh.copy()"
   ]
  },
  {
   "cell_type": "markdown",
=======
   "id": "3a9d0019",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "## Feature 하나씩 빼면서 성능 체크"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "metadata": {},
   "source": [
    "0번부터 38번 변수 하나씩 빼면서 성능 체크했음 >> 37번 변수를 빼고 모델을 돌려서 0.7084로 가장 작은 Loss값이 나왔으니 37번 변수는 필요없는 변수임<br>\n",
    "두 번째 필요없는 변수, 세 번째 필요없는 변수 같이 빼가면서 튜닝된 lgbm에 넣고 성능 체크해보기 "
=======
   "id": "5bbb3bb4",
   "metadata": {},
   "source": [
    "변수 하나씩 제거하면서 성능 체크<br>\n",
    "제거하여 성능이 좋게 나온 것들은 리스트에 따로 저장해두기"
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T13:10:50.124789Z",
     "start_time": "2021-05-16T13:06:20.254928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) multi_logloss: 0.7091039811710851\n",
      "(1,) multi_logloss: 0.707535713833363\n",
      "(2,) multi_logloss: 0.7096705610517619\n",
      "(3,) multi_logloss: 0.7075603127362105\n",
      "(4,) multi_logloss: 0.710036426695277\n",
      "(5,) multi_logloss: 0.7089164528385543\n",
      "(6,) multi_logloss: 0.7102052332171461\n",
      "(7,) multi_logloss: 0.7101579052118886\n",
      "(8,) multi_logloss: 0.7083876735870491\n",
      "(9,) multi_logloss: 0.7089168046787044\n",
      "(10,) multi_logloss: 0.7110345481646222\n",
      "(11,) multi_logloss: 0.7083915025266401\n",
      "(12,) multi_logloss: 0.7093554521430614\n",
      "(13,) multi_logloss: 0.7095354167728702\n",
      "(14,) multi_logloss: 0.7089509348433582\n",
      "(15,) multi_logloss: 0.711251820992898\n",
      "(16,) multi_logloss: 0.7087739461914122\n",
      "(17,) multi_logloss: 0.771469083845089\n",
      "(18,) multi_logloss: 0.7108667885272036\n",
      "(19,) multi_logloss: 0.7104278270419906\n",
      "(20,) multi_logloss: 0.7093739982356556\n",
      "(21,) multi_logloss: 0.7095671241506042\n",
      "(22,) multi_logloss: 0.7096668568162048\n",
      "(23,) multi_logloss: 0.709633094923249\n",
      "(24,) multi_logloss: 0.7088326042400971\n",
      "(25,) multi_logloss: 0.7105497250023582\n",
      "(26,) multi_logloss: 0.7091700755748547\n",
      "(27,) multi_logloss: 0.709198278996982\n",
      "(28,) multi_logloss: 0.7088986142642686\n",
      "(29,) multi_logloss: 0.7088782186099765\n",
      "(30,) multi_logloss: 0.7096144644715937\n",
      "(31,) multi_logloss: 0.7099205553401796\n",
      "(32,) multi_logloss: 0.7084177759623502\n",
      "(33,) multi_logloss: 0.709391606302738\n",
      "(34,) multi_logloss: 0.7091290097570873\n",
      "(35,) multi_logloss: 0.7097872589976196\n",
      "(36,) multi_logloss: 0.7091216487978842\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 2):\n",
    "    for j in combinations(list(range(0, train_x.shape[1]-1)), i):\n",
=======
   "execution_count": null,
   "id": "d7005902",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "cv = np.zeros((train_x.shape[0], 3))\n",
    "for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "    x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "    y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "    lgbm = LGBMClassifier(n_estimators=1000, objective='multiclass')\n",
    "    lgbm.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None)\n",
    "    cv[val_idx] = lgbm.predict_proba(x_val)\n",
    "print(f'Initial_multi_logloss: a{log_loss(train_y, cv)}')\n",
    "remove_features = []\n",
    "for i in range(1, 2):\n",
    "    for j in combinations(list(range(0, train_x.shape[1])), i): # combination함수 : 피쳐 0부터 하나씩 빼면서 모델 성능을 체크해봄\n",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "        train_new_x = train_x.drop(train_x.columns[list(j)], axis=1)\n",
    "        \n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        cv = np.zeros((train_new_x.shape[0], 3))\n",
    "        for n, (train_idx, val_idx) in enumerate(kfold.split(train_new_x, train_y)):\n",
    "            x_train, x_val = train_new_x.iloc[train_idx], train_new_x.iloc[val_idx]\n",
    "            y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "            lgbm = LGBMClassifier(n_estimators=1000, objective='multiclass')\n",
    "            lgbm.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None)\n",
    "            cv[val_idx] = lgbm.predict_proba(x_val)\n",
<<<<<<< HEAD
    "        print(f'{j} multi_logloss: {log_loss(train_y, cv)}')"
=======
    "        print(f'{j} multi_logloss: {log_loss(train_y, cv)}')\n",
    "        if log_loss(train_y, cv)<0.708:\n",
    "            remove_features.append(list(j)[0])"
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "metadata": {},
   "source": [
    "## A/B Test 하고 필요없는 변수 지우는 코드"
=======
   "id": "b9e94de3",
   "metadata": {},
   "source": [
    "## 저장한 변수 지우는 코드"
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(train.columns[[0, 1]], axis=1) # 만약 0번째, 1번째 변수를 빼고싶으면 이렇게 빼면 됨\n",
    "test = test.drop(test.columns[[0, 1]], axis=1)"
=======
   "id": "93eb2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features = [1, 3, 4, 8, 11, 32]\n",
    "train = train.drop(train.columns[remove_features], axis=1)\n",
    "test = test.drop(test.columns[remove_features], axis=1)"
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "ec41df66",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "## 다시 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "70c70fad",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oh = train.copy()\n",
    "train_noh = train.copy()\n",
    "test_oh = test.copy()\n",
    "test_noh = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "90b7705d",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "object_col = []\n",
    "for col in train_noh.columns:\n",
    "    if train_noh[col].dtype == 'object':\n",
    "        train_noh[col] = train_noh[col].astype('category')\n",
    "        test_noh[col] = test_noh[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "6b0d11c6",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "object_col = []\n",
    "for col in train_oh.columns:\n",
    "    if train_oh[col].dtype == 'object':\n",
    "        object_col.append(col)\n",
    "print(object_col)        \n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train_oh.drop(object_col, axis=1, inplace=True)\n",
    "train_oh = pd.concat([train_oh, train_onehot_df], axis=1)    \n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test_oh.drop(object_col, axis=1, inplace=True)\n",
    "test_oh = pd.concat([test_oh, test_onehot_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "9170c227",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "# 6. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "44c41a3a",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "pred_test_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "2dd1fc0e",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "## (1) Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "57f5543e",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_noh.drop(['credit'], axis=1)\n",
    "train_y = train_noh['credit']\n",
    "test_x = test_noh.copy()"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "70bc62e8",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperopt"
=======
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=42\n",
    "\n",
    "# Hyperopt의 metric함수를 StratifiedKFold(cv=5)로 구하기\n",
    "def score(params):\n",
    "    print(\"Training with params: \")\n",
    "    print(params)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, random_state = SEED, shuffle = True)\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        lgbmodel = LGBMClassifier(**params)\n",
    "\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=100) \n",
    "        cv[val_idx, :] = lgbmodel.predict_proba(x_val)\n",
    "        print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}')\n",
    "    print('multi_logloss:', log_loss(train_y, cv))\n",
    "    score = log_loss(train_y, cv)\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "# Hyperopt의 범위를 지정해주고 max_evals만큼 반복한 후 최적의 파라미터를 반환\n",
    "def optimize(random_state=SEED):\n",
    "    \n",
    "#     param = {'objective':'multi:softprob', 'seed':SEED, 'num_class': 3, 'eval_metric':'mlogloss', \n",
    "#          'eta': 0.01, 'min_child_weight': 3,\n",
    "#          'colsample_bytree': 0.3, 'colsample_bylevel': 0.6, 'subsample': 0.8\n",
    "#         }\n",
    "    space = {\n",
    "        'learning_rate': hp.quniform('learning_rate', 0.003, 0.006, 0.001),  #  0.003, 0.006 범위 지정\n",
    "        #'learning_rate' : 0.005, 고정하고 싶을 때\n",
    "        'num_leaves': scope.int(hp.quniform('num_leaves', 1000, 1200, 50)),\n",
    "        'num_leaves' : 1000,\n",
    "        #'min_child_weight': hp.quniform('min_child_weight', 1, 3, 1),\n",
    "        'min_child_weight' : 2,\n",
    "        #'subsample': hp.quniform('subsample', 0.8, 1, 0.05),\n",
    "        'subsample' : 1,\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.3, 0.7, 0.05),\n",
    "        #'colsample_bytree' : 0.4,\n",
    "        'max_depth' : -1,\n",
    "        'n_estimators' : 5000,\n",
    "        'objective' : 'multiclass',\n",
    "        'num_class' : 3,\n",
    "        'seed': SEED,\n",
    "    }\n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters\n",
    "    best = fmin(score, space, algo=tpe.suggest, \n",
    "                # trials=trials, \n",
    "                max_evals=12)\n",
    "    return best\n",
    "\n",
    "best_hyperparams = optimize()\n",
    "print(\"The best hyperparameters are: \", \"\\n\")\n",
    "print(best_hyperparams)"
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "601048b4",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "### 3 seeds x 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "8cb495fc",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[42,2019,91373] # Lucky seed 늘려가면서 하기\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # CV 늘려가면서 하기\n",
    "    cv=np.zeros((train_x.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        lgbmodel = LGBMClassifier(learning_rate=0.005, objective='multiclass', n_estimators=10000, num_leaves=1000, \n",
<<<<<<< HEAD
    "                                  max_depth=-1, min_child_weight=2, colsample_bytree=0.4,  \n",
=======
    "                                  max_depth=-1, min_child_weight=2, colsample_bytree=0.55,  \n",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "                                   n_jobs=-1, random_state=seed)\n",
    "\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=100) \n",
    "        #joblib.dump(lgbmodel, f'./pred_pkl/LGB_{n+1}_fold_{seed}_seed_lgb.pkl')\n",
    "\n",
    "        # CROSS-VALIDATION , EVALUATE CV\n",
    "        cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        pred_test += lgbmodel.predict_proba(test_x) / 10 # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "    pred_dict['lgb'+str(i+1)] = cv\n",
    "    pred_test_dict['lgb'+str(i+1)] = pred_test\n",
    "        \n",
    "    print('multi_logloss :', log_loss(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "d4f6038b",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "lgbmodels_path = os.listdir('./pred_pkl/')\n",
    "lgbmodels_list = [x for x in lgbmodels_path if x.endswith(\"lgb.pkl\")]\n",
    "assert len(lgbmodels_list) == 15\n",
    "lgb_preds = np.zeros((test_x.shape[0], 3))\n",
    "\n",
    "for m in lgbmodels_list:\n",
    "    lgbmodel = joblib.load('./pred_pkl/'+m)\n",
    "    lgb_preds_proba = lgbmodel.predict_proba(test)\n",
    "    lgb_preds += lgb_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "e246c33b",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "## (2) XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "087af2c9",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "원핫인코딩된 feature로 만들어주기 **꼭 밑에 코드 실행하고 XGBoost랑 Randomforest 돌리기!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "08bf68c9",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_oh.drop(['credit'], axis=1)\n",
    "train_y = train_oh['credit']\n",
    "test_x = test_oh.copy()"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "4c1996f9",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "### Parameter Tuning (hyperopt)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "cf691102",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "이거 오래걸리므로 안해도됨, 그리고 이미 했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "2f55463c",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=0\n",
    "\n",
    "# Hyperopt의 metric함수를 StratifiedKFold(cv=5)로 구하기\n",
    "def score(params):\n",
    "    print(\"Training with params: \")\n",
    "    print(params)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, random_state = SEED, shuffle = True)\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "        xgbmodel = xgb.train(params, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "        cv[val_idx, :] = xgbmodel.predict(dvalid)\n",
    "        print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}')\n",
    "    print('multi_logloss:', log_loss(train_y, cv))\n",
    "    score = log_loss(train_y, cv)\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "# Hyperopt의 범위를 지정해주고 max_evals만큼 반복한 후 최적의 파라미터를 반환\n",
    "def optimize(random_state=SEED):\n",
    "    \n",
<<<<<<< HEAD
    "    param = {'objective':'multi:softprob', 'seed':SEED, 'num_class': 3, 'eval_metric':'mlogloss', \n",
    "         'eta': 0.01, 'min_child_weight': 3,\n",
    "         'colsample_bytree': 0.3, 'colsample_bylevel': 0.6, 'subsample': 0.8\n",
    "        }\n",
    "    space = {\n",
    "        'eta': hp.quniform('eta', 0.003, 0.006, 0.001),\n",
    "        # A problem with max_depth casted to float instead of int with\n",
    "        # the hp.quniform method.\n",
=======
    "#     param = {'objective':'multi:softprob', 'seed':SEED, 'num_class': 3, 'eval_metric':'mlogloss', \n",
    "#          'eta': 0.01, 'min_child_weight': 3,\n",
    "#          'colsample_bytree': 0.3, 'colsample_bylevel': 0.6, 'subsample': 0.8\n",
    "#         }\n",
    "    space = {\n",
    "        'eta': hp.quniform('eta', 0.003, 0.006, 0.001),\n",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "        #'max_depth':  hp.choice('max_depth', np.arange(20, 30, dtype=int)),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 3, 1),\n",
    "        'subsample': hp.quniform('subsample', 0.6, 0.8, 0.05),\n",
    "        'gamma': hp.quniform('gamma', 0.6, 0.8, 0.05),\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.3, 0.7, 0.05),\n",
    "        'colsample_bylevel': hp.quniform('colsample_bylevel', 0.3, 0.7, 0.05),\n",
    "        'max_depth' : 100,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'objective' : 'multi:softprob',\n",
<<<<<<< HEAD
    "        # Increase this number if you have more cores. Otherwise, remove it and it will default \n",
    "        # to the maxium number.\n",
    "        # 'tree_method' : 'gpu_hist',\n",
=======
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "        'num_class' : 3,\n",
    "        'seed': SEED,\n",
    "    }\n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters\n",
    "    best = fmin(score, space, algo=tpe.suggest, \n",
    "                # trials=trials, \n",
    "                max_evals=10)\n",
    "    return best\n",
    "\n",
<<<<<<< HEAD
    "#-------------------------------------------------#\n",
    "\n",
    "\n",
    "# Load processed data\n",
    "\n",
    "# You could use the following script to generate a well-processed train and test data sets:\n",
    "# https://www.kaggle.com/yassinealouini/predicting-red-hat-business-value/features-processing\n",
    "# I have only used the .head() of the data sets since the process takes a long time to run.\n",
    "# I have also put the act_train and act_test data sets since I don't have the processed data sets \n",
    "# loaded. \n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------#\n",
    "\n",
    "# Run the optimization\n",
    "\n",
    "# Trials object where the history of search will be stored\n",
    "# For the time being, there is a bug with the following version of hyperopt.\n",
    "# You can read the error messag on the log file.\n",
    "# For the curious, you can read more about it here: https://github.com/hyperopt/hyperopt/issues/234\n",
    "# => So I am commenting it.\n",
    "# trials = Trials()\n",
    "\n",
=======
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "best_hyperparams = optimize()\n",
    "print(\"The best hyperparameters are: \", \"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "2884fd2c",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "### 3 seeds x 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "96f2441e",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[42, 2019, 91373] # 늘려가면서\n",
    "xgtest = xgb.DMatrix(test_x)\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # 늘려가면서\n",
    "    cv = np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "        param = {'colsample_bylevel': 0.5, 'colsample_bytree': 0.4, 'eta': 0.003, 'eval_metric': 'mlogloss', \n",
    "         'gamma': 0.7, 'max_depth': 100, 'min_child_weight': 2.0, 'num_class': 3, \n",
<<<<<<< HEAD
    "         'objective': 'multi:softprob', 'seed': 0, 'subsample': 0.7000000000000001}\n",
=======
    "         'objective': 'multi:softprob', 'seed': 0, 'subsample': 0.7}\n",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "        xgbmodel = xgb.train(param, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "        #joblib.dump(xgbmodel, f'./pred_pkl/XGB_{n+1}_fold_{seed}_seed_xgb.pkl')\n",
    "\n",
    "        cv[val_idx, :] = xgbmodel.predict(dvalid)\n",
<<<<<<< HEAD
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}')\n",
=======
    "        print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}')\n",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "        pred_test += xgbmodel.predict(xgtest) / 10 # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "        \n",
    "    pred_dict['xgb'+str(i+1)] = cv\n",
    "    pred_test_dict['xgb'+str(i+1)] = pred_test\n",
    "    print('multi_logloss:', log_loss(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "db598b80",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "xgbmodels_path = os.listdir('./pred_pkl/')\n",
    "xgbmodels_list = [x for x in xgbmodels_path if x.endswith(\"xgb.pkl\")]\n",
    "assert len(xgbmodels_list) == 15\n",
    "xgb_preds = np.zeros((test_x.shape[0], 3))\n",
    "xgtest = xgb.DMatrix(test_X)\n",
    "\n",
    "for m in xgbmodels_list:\n",
    "    xgbmodel = joblib.load('./pred_pkl/'+m)\n",
    "    xgb_preds_proba = xgbmodel.predict_proba(xgtest\n",
    "       xgb_preds += xgb_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "8b15d010",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "## (3) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "6dab24e6",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "f1fdf172",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {
    "collapsed": true
   },
   "source": [
    "params = {'max_depth': [55, 60, 65] # 튜닝할 파라미터 삽입\n",
    "            }\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state = 0, n_estimators = 1000, \n",
    "                                min_samples_leaf=2, min_samples_split=2,\n",
    "                                criterion='entropy', n_jobs = -1)\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid = params, cv = 5, n_jobs = -1)\n",
    "grid_cv.fit(df_train, y)\n",
    "\n",
    "print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "96cbb2eb",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "### 3 seeds, 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "e7d6db95",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[42,2019,91373] # 늘려가면서\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # 늘려가면서\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
<<<<<<< HEAD
    "        rfmodel = RandomForestClassifier(n_estimators=1000, criterion='entropy', max_depth=55,\n",
=======
    "        rfmodel = RandomForestClassifier(n_estimators=1200, criterion='entropy', max_depth=60,\n",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "                                         min_samples_leaf=2, min_samples_split=2,\n",
    "                                         random_state=seed)\n",
    "        rfmodel.fit(x_train, y_train)\n",
    "        #joblib.dump(rfmodel, f'./pred_pkl/RF_{n+1}_fold_{seed}_seed_rf.pkl')\n",
    "        \n",
    "        cv[val_idx, :] = rfmodel.predict_proba(x_val)      \n",
<<<<<<< HEAD
    "        #print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}')\n",
=======
    "        print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}')\n",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
    "        pred_test += rfmodel.predict_proba(test_x) / 10 # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "        \n",
    "    pred_dict['rf'+str(i+1)] = cv\n",
    "    pred_test_dict['rf'+str(i+1)] = pred_test\n",
    "    print('multi_logloss :', log_loss(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "4479d291",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "rfmodels_path = os.listdir('./pred_pkl/')\n",
    "rfmodels_list = [x for x in rfmodels_path if x.endswith(\"rf.pkl\")]\n",
    "assert len(rfmodels_list) == 15\n",
    "rf_preds = np.zeros((test_x.shape[0], 3))\n",
    "\n",
    "for m in rfmodels_list:\n",
    "    rfmodel = joblib.load('./pred_pkl/'+m)\n",
    "    rf_preds_proba = rfmodel.predict_proba(test_x)\n",
    "    rf_preds += rf_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "c26bd1f1",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "## (4) Catboost (성능X)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "6f5d2cc4",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "lucky_seeds=[42,2019,91373]\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "    cv = np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        _train = Pool(x_train, label=y_train)\n",
    "        _valid = Pool(x_val, label=y_val)\n",
    "\n",
    "        catmodel =  CatBoostClassifier(loss_function='MultiClass', early_stopping_rounds=50, \n",
    "                                       random_state=seed, learning_rate=0.02, iterations=100000\n",
    "                                       #task_type=\"GPU\"\n",
    "                                      )\n",
    "        \n",
    "        catmodel.fit(_train, eval_set=_valid, use_best_model=True, verbose=2000)\n",
    "        #joblib.dump(rfmodel, f'./pred_pkl/RF_{n+1}_fold_{seed}_seed_rf.pkl')\n",
    "        \n",
    "        cv[val_idx, :] = catmodel.predict_proba(x_val)        \n",
    "        pred_test += catmodel.predict_proba(test_x) / 5\n",
    "        \n",
    "    pred_dict['cat'+str(i+1)] = cv\n",
    "    pred_test_dict['cat'+str(i+1)] = pred_test\n",
    "    print('multi_logloss :', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "5b6a40d1",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "## (4) Stacking (AutoLGB)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "5eb06064",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "### 27features = 3seed(42, 2019, 91373) x 3model(lgb, xgb, rf) x 3class(0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "9c98a940",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(np.hstack([x for _, x in pred_dict.items()]))\n",
    "X_test = pd.DataFrame(np.hstack([x for _, x in pred_test_dict.items()]))\n",
    "\n",
    "pred = np.zeros((X_train.shape[0], 3), dtype=float)\n",
    "pred_test = np.zeros((X_test.shape[0], 3), dtype=float)\n",
    "#kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42) # 이건 CV 너무 크게하면 안됨, 3~6까지 테스트해보면 좋을듯\n",
    "\n",
    "for i_cv, (i_trn, i_val) in enumerate(cv.split(X_train, train_y)):\n",
    "    if i_cv == 0:\n",
    "        clf = AutoLGB(objective='multiclass', metric='multi_logloss', params={'num_class': 3}, \n",
    "                      feature_selection=False, n_est=10000)\n",
    "        clf.tune(X_train.iloc[i_trn], train_y[i_trn])\n",
    "        n_best = clf.n_best\n",
    "        features = clf.features\n",
    "        params = clf.params\n",
    "        print(f'best iteration: {n_best}')\n",
    "        print(f'selected features ({len(features)}): {features}')        \n",
    "        print(params)\n",
    "        clf.fit(X_train.iloc[i_trn], train_y[i_trn])\n",
    "    else:\n",
    "        train_data = lgb.Dataset(X_train[features].iloc[i_trn], label=train_y[i_trn])\n",
    "        clf = lgb.train(params, train_data, n_best, verbose_eval=100)\n",
    "    \n",
    "    pred[i_val] = clf.predict(X_train[features].iloc[i_val])\n",
    "    pred_test += clf.predict(X_test[features]) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "47aa2c88",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'CV Log Loss: {log_loss(train_y, pred):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "a6eb4980",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "source": [
    "# 결과 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "a0b2e025",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_submission.copy()\n",
    "submission.iloc[:, 1:] = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "9264668d",
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
<<<<<<< HEAD
   "version": "3.8.3"
=======
   "version": "3.8.0"
>>>>>>> 7d14cd2ca95599f2bd92b49dd544b9a392b883d6
=======
   "version": "3.8.0"
>>>>>>> Stashed changes
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
