{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e515013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from kaggler.model import AutoLGB\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, log_loss\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db463649",
   "metadata": {},
   "source": [
    "# 1. 문제 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b57cb5",
   "metadata": {},
   "source": [
    "# 2. 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c78c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fbbe7bc",
   "metadata": {},
   "source": [
    "## (1) 데이콘 기본 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c526372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv').drop(['index'], axis=1).fillna('NAN')\n",
    "test = pd.read_csv('data/test.csv').drop(['index'], axis=1).fillna('NAN')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e15bd",
   "metadata": {},
   "source": [
    "# 4. 탐색적 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8702ae4c",
   "metadata": {},
   "source": [
    "# 5. 변수 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf2fc42",
   "metadata": {},
   "source": [
    "## (1) 이상치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80a66380",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_col = []\n",
    "for col in train.columns:\n",
    "    if (train[col].dtype == 'O'):\n",
    "        object_col.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4ab12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train.loc[:, object_col]).toarray(), \n",
    "                               columns=enc.get_feature_names(object_col))\n",
    "train.drop(object_col, axis=1, inplace=True)\n",
    "train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test.loc[:, object_col]).toarray(),\n",
    "                              columns=enc.get_feature_names(object_col))\n",
    "test.drop(object_col, axis=1, inplace=True)\n",
    "test = pd.concat([test, test_onehot_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f471ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAYS_BIRTH\n",
    "train['DAYS_BIRTH_month']=np.floor((-train['DAYS_BIRTH'])/30)-(\n",
    "    (np.floor((-train['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "train['DAYS_BIRTH_week']=np.floor((-train['DAYS_BIRTH'])/7)-(\n",
    "    (np.floor((-train['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_EMPLOYED\n",
    "train['DAYS_EMPLOYED_month']=np.floor((-train['DAYS_EMPLOYED'])/30)-(\n",
    "    (np.floor((-train['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "train['DAYS_EMPLOYED_week']=np.floor((-train['DAYS_EMPLOYED'])/7)-(\n",
    "    (np.floor((-train['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# before_EMPLOYED\n",
    "train['before_EMPLOYED']=train['DAYS_BIRTH']-train['DAYS_EMPLOYED']\n",
    "train['before_EMPLOYED_month']=np.floor((-train['before_EMPLOYED'])/30)-(\n",
    "    (np.floor((-train['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "train['before_EMPLOYED_week']=np.floor((-train['before_EMPLOYED'])/7)-(\n",
    "    (np.floor((-train['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_BIRTH\n",
    "test['DAYS_BIRTH_month']=np.floor((-test['DAYS_BIRTH'])/30)-(\n",
    "    (np.floor((-test['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "test['DAYS_BIRTH_week']=np.floor((-test['DAYS_BIRTH'])/7)-(\n",
    "    (np.floor((-test['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_EMPLOYED\n",
    "test['DAYS_EMPLOYED_month']=np.floor((-test['DAYS_EMPLOYED'])/30)-(\n",
    "    (np.floor((-test['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "test['DAYS_EMPLOYED_week']=np.floor((-test['DAYS_EMPLOYED'])/7)-(\n",
    "    (np.floor((-test['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# before_EMPLOYED\n",
    "test['before_EMPLOYED']=test['DAYS_BIRTH']-test['DAYS_EMPLOYED']\n",
    "test['before_EMPLOYED_month']=np.floor((-test['before_EMPLOYED'])/30)-(\n",
    "    (np.floor((-test['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "test['before_EMPLOYED_week']=np.floor((-test['before_EMPLOYED'])/7)-(\n",
    "    (np.floor((-test['before_EMPLOYED'])/7)/4).astype(int)*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ae58425",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 아이의 수가 7명 이상인 데이터 제거\n",
    "train = train[train['child_num']<=6].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d29608f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 수익 skewed data 처리\n",
    "train['log_income_total'] = np.log(train['income_total'])\n",
    "train['sqrt_income_total'] = np.sqrt(train['income_total'])\n",
    "train['boxcox_income_total'] = stats.boxcox(train['income_total'])[0]\n",
    "test['log_income_total'] = np.log(test['income_total'])\n",
    "test['sqrt_income_total'] = np.sqrt(test['income_total'])\n",
    "test['boxcox_income_total'] = stats.boxcox(test['income_total'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a2f73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train.drop(['credit'], axis=1)\n",
    "train_y = train['credit']\n",
    "test_x = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9428cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = train[['credit']]\n",
    "true['0'] = true['credit'][true['credit']==0.0]\n",
    "true['1'] = true['credit'][true['credit']==1.0]\n",
    "true['2'] = true['credit'][true['credit']==2.0]\n",
    "del true['credit']\n",
    "true = true.replace([0.0, 2.0], [1.0, 1.0])\n",
    "true = true.fillna(0)\n",
    "true = true.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca20b5",
   "metadata": {},
   "source": [
    "# 6. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0122bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "pred_test_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e69601",
   "metadata": {},
   "source": [
    "## (1) Lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea62c2",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc83a9a5",
   "metadata": {},
   "source": [
    "### 3 seeds x 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1de18ec7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.798839\n",
      "[200]\tvalid_0's multi_logloss: 0.778206\n",
      "[300]\tvalid_0's multi_logloss: 0.76643\n",
      "[400]\tvalid_0's multi_logloss: 0.757688\n",
      "[500]\tvalid_0's multi_logloss: 0.750063\n",
      "[600]\tvalid_0's multi_logloss: 0.743378\n",
      "[700]\tvalid_0's multi_logloss: 0.737894\n",
      "[800]\tvalid_0's multi_logloss: 0.733222\n",
      "[900]\tvalid_0's multi_logloss: 0.728691\n",
      "[1000]\tvalid_0's multi_logloss: 0.725023\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's multi_logloss: 0.725023\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.798262\n",
      "[200]\tvalid_0's multi_logloss: 0.77794\n",
      "[300]\tvalid_0's multi_logloss: 0.767207\n",
      "[400]\tvalid_0's multi_logloss: 0.758899\n",
      "[500]\tvalid_0's multi_logloss: 0.753058\n",
      "[600]\tvalid_0's multi_logloss: 0.747388\n",
      "[700]\tvalid_0's multi_logloss: 0.742806\n",
      "[800]\tvalid_0's multi_logloss: 0.739606\n",
      "[900]\tvalid_0's multi_logloss: 0.735866\n",
      "[1000]\tvalid_0's multi_logloss: 0.733276\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's multi_logloss: 0.733276\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.818488\n",
      "[200]\tvalid_0's multi_logloss: 0.800896\n",
      "[300]\tvalid_0's multi_logloss: 0.790474\n",
      "[400]\tvalid_0's multi_logloss: 0.782403\n",
      "[500]\tvalid_0's multi_logloss: 0.776536\n",
      "[600]\tvalid_0's multi_logloss: 0.771324\n",
      "[700]\tvalid_0's multi_logloss: 0.767013\n",
      "[800]\tvalid_0's multi_logloss: 0.763827\n",
      "[900]\tvalid_0's multi_logloss: 0.760239\n",
      "[1000]\tvalid_0's multi_logloss: 0.756749\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's multi_logloss: 0.756749\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.81359\n",
      "[200]\tvalid_0's multi_logloss: 0.794369\n",
      "[300]\tvalid_0's multi_logloss: 0.785191\n",
      "[400]\tvalid_0's multi_logloss: 0.775858\n",
      "[500]\tvalid_0's multi_logloss: 0.76884\n",
      "[600]\tvalid_0's multi_logloss: 0.762726\n",
      "[700]\tvalid_0's multi_logloss: 0.757643\n",
      "[800]\tvalid_0's multi_logloss: 0.753484\n",
      "[900]\tvalid_0's multi_logloss: 0.75048\n",
      "[1000]\tvalid_0's multi_logloss: 0.747112\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's multi_logloss: 0.747112\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.813307\n",
      "[200]\tvalid_0's multi_logloss: 0.797386\n",
      "[300]\tvalid_0's multi_logloss: 0.787351\n",
      "[400]\tvalid_0's multi_logloss: 0.777926\n",
      "[500]\tvalid_0's multi_logloss: 0.771541\n",
      "[600]\tvalid_0's multi_logloss: 0.765895\n",
      "[700]\tvalid_0's multi_logloss: 0.760327\n",
      "[800]\tvalid_0's multi_logloss: 0.756144\n",
      "[900]\tvalid_0's multi_logloss: 0.751897\n",
      "[1000]\tvalid_0's multi_logloss: 0.748955\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's multi_logloss: 0.748955\n",
      "multi_logloss : 0.7422223491879912\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.794739\n",
      "[200]\tvalid_0's multi_logloss: 0.776451\n",
      "[300]\tvalid_0's multi_logloss: 0.767028\n",
      "[400]\tvalid_0's multi_logloss: 0.759139\n",
      "[500]\tvalid_0's multi_logloss: 0.752916\n",
      "[600]\tvalid_0's multi_logloss: 0.747227\n",
      "[700]\tvalid_0's multi_logloss: 0.742526\n",
      "[800]\tvalid_0's multi_logloss: 0.738359\n",
      "[900]\tvalid_0's multi_logloss: 0.734056\n",
      "[1000]\tvalid_0's multi_logloss: 0.730572\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's multi_logloss: 0.730572\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.816077\n",
      "[200]\tvalid_0's multi_logloss: 0.797388\n",
      "[300]\tvalid_0's multi_logloss: 0.785976\n",
      "[400]\tvalid_0's multi_logloss: 0.777921\n",
      "[500]\tvalid_0's multi_logloss: 0.771724\n",
      "[600]\tvalid_0's multi_logloss: 0.765625\n",
      "[700]\tvalid_0's multi_logloss: 0.760654\n",
      "[800]\tvalid_0's multi_logloss: 0.756808\n",
      "[900]\tvalid_0's multi_logloss: 0.752967\n",
      "[1000]\tvalid_0's multi_logloss: 0.749783\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's multi_logloss: 0.749774\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.815005\n",
      "[200]\tvalid_0's multi_logloss: 0.795021\n",
      "[300]\tvalid_0's multi_logloss: 0.783971\n",
      "[400]\tvalid_0's multi_logloss: 0.775044\n",
      "[500]\tvalid_0's multi_logloss: 0.768938\n",
      "[600]\tvalid_0's multi_logloss: 0.763614\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-67ef508f76f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                    n_jobs=-1, random_state=seed)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlgbmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m#joblib.dump(lgbmodel, f'./pred_pkl/LGB_{n+1}_fold_{seed}_seed_lgb.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         super(LGBMClassifier, self).fit(X, _y, sample_weight=sample_weight,\n\u001b[0m\u001b[1;32m    848\u001b[0m                                         \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                                         \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0minit_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         self._Booster = train(params, train_set,\n\u001b[0m\u001b[1;32m    613\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    250\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   2456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot update due to null objective function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2458\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2459\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2460\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lucky_seeds=[42,2019,91373]\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "    cv=np.zeros((train.shape[0], 3))\n",
    "    #pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        lgbmodel = LGBMClassifier(learning_rate=0.01, objective='multiclass', n_estimators=1000,\n",
    "                                   n_jobs=-1, random_state=seed)\n",
    "\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "        #joblib.dump(lgbmodel, f'./pred_pkl/LGB_{n+1}_fold_{seed}_seed_lgb.pkl')\n",
    "\n",
    "        # CROSS-VALIDATION , EVALUATE CV\n",
    "        cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        #pred_test += lgbmodel.predict_proba(test_x) / 5\n",
    "    #pred_dict['lgb'+str(i+1)] = cv\n",
    "    #pred_test_dict['lgb'+str(i+1)] = pred_test\n",
    "        \n",
    "    print('multi_logloss :', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5547d79",
   "metadata": {},
   "source": [
    "# 여기까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c11e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7431e179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c5ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b03420e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750cbf23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc24fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d1174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b0139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8cf140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_logloss : 0.7015993077380257\n",
      "multi_logloss : 0.7008750060407489\n",
      "multi_logloss : 0.7009575587001414\n"
     ]
    }
   ],
   "source": [
    "lucky_seeds=[42,2019,91373]\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "    cv=np.zeros((train.shape[0], 3))\n",
    "    #pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        lgbmodel = LGBMClassifier(learning_rate=0.005, objective='multiclass', n_estimators=10000, num_leaves=1000, \n",
    "                                  max_depth=-1, min_child_weight=2, colsample_bytree=0.4,  \n",
    "                                   n_jobs=-1, random_state=seed)\n",
    "\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "        #joblib.dump(lgbmodel, f'./pred_pkl/LGB_{n+1}_fold_{seed}_seed_lgb.pkl')\n",
    "\n",
    "        # CROSS-VALIDATION , EVALUATE CV\n",
    "        cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        #pred_test += lgbmodel.predict_proba(test_x) / 5\n",
    "    #pred_dict['lgb'+str(i+1)] = cv\n",
    "    #pred_test_dict['lgb'+str(i+1)] = pred_test\n",
    "        \n",
    "    print('multi_logloss :', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bbf346",
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[42,2019,91373]\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "    cv=np.zeros((train.shape[0], 3))\n",
    "    #pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        lgbmodel = LGBMClassifier(learning_rate=0.005, objective='multiclass', n_estimators=10000, num_leaves=800, \n",
    "                                  max_depth=-1, min_child_weight=2, colsample_bytree=0.4,  \n",
    "                                   n_jobs=-1, random_state=seed)\n",
    "\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "        #joblib.dump(lgbmodel, f'./pred_pkl/LGB_{n+1}_fold_{seed}_seed_lgb.pkl')\n",
    "\n",
    "        # CROSS-VALIDATION , EVALUATE CV\n",
    "        cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        #pred_test += lgbmodel.predict_proba(test_x) / 5\n",
    "    #pred_dict['lgb'+str(i+1)] = cv\n",
    "    #pred_test_dict['lgb'+str(i+1)] = pred_test\n",
    "        \n",
    "    print('multi_logloss :', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a16a9be2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-44585b4fe7fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    n_jobs=-1, random_state=seed)\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mlgbmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#joblib.dump(lgbmodel, f'./pred_pkl/LGB_{n+1}_fold_{seed}_seed_lgb.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         super(LGBMClassifier, self).fit(X, _y, sample_weight=sample_weight,\n\u001b[0m\u001b[1;32m    848\u001b[0m                                         \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                                         \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0minit_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         self._Booster = train(params, train_set,\n\u001b[0m\u001b[1;32m    613\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    250\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   2456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot update due to null objective function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2458\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2459\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2460\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lucky_seeds=[42,2019,91373]\n",
    "\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "    cv=np.zeros((train.shape[0], 3))\n",
    "    #pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        lgbmodel = LGBMClassifier(learning_rate=0.005, objective='multiclass', n_estimators=10000, num_leaves=600, \n",
    "                                  max_depth=-1, min_child_weight=2, colsample_bytree=0.4,  \n",
    "                                   n_jobs=-1, random_state=seed)\n",
    "\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None) \n",
    "        #joblib.dump(lgbmodel, f'./pred_pkl/LGB_{n+1}_fold_{seed}_seed_lgb.pkl')\n",
    "\n",
    "        # CROSS-VALIDATION , EVALUATE CV\n",
    "        cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        #pred_test += lgbmodel.predict_proba(test_x) / 5\n",
    "    #pred_dict['lgb'+str(i+1)] = cv\n",
    "    #pred_test_dict['lgb'+str(i+1)] = pred_test\n",
    "        \n",
    "    print('multi_logloss :', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa748213",
   "metadata": {},
   "source": [
    "lgbmodels_path = os.listdir('./pred_pkl/')\n",
    "lgbmodels_list = [x for x in lgbmodels_path if x.endswith(\"lgb.pkl\")]\n",
    "assert len(lgbmodels_list) == 15\n",
    "lgb_preds = np.zeros((test_x.shape[0], 3))\n",
    "\n",
    "for m in lgbmodels_list:\n",
    "    lgbmodel = joblib.load('./pred_pkl/'+m)\n",
    "    lgb_preds_proba = lgbmodel.predict_proba(test)\n",
    "    lgb_preds += lgb_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f156ef",
   "metadata": {},
   "source": [
    "## (2) XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90efbcab",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882c80c",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "params = {'learning_rate':[0.01, 0.005],\n",
    "          'max_depth': [30, 35, 40] # 튜닝할 파라미터 삽입\n",
    "            }\n",
    "\n",
    "xgb_clf = XGBClassifier(n_estimators=100, min_child_weight=2, \n",
    "                        colsample_bytree=0.8, colsample_bylevel=0.8, subsample=0.8,\n",
    "                        num_class=3, objective='multiclass', n_jobs=-1)\n",
    "\n",
    "grid_cv = GridSearchCV(xgb_clf, param_grid=params, cv=5, n_jobs=-1)\n",
    "grid_cv.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565fc8b",
   "metadata": {},
   "source": [
    "hr_grid_df = pd.DataFrame(grid_cv.cv_results_)\n",
    "hr_grid_df.loc[:, ['mean_test_score', \"params\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db740ef5",
   "metadata": {},
   "source": [
    "### 3 seeds x 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[42, 2019, 91373]\n",
    "xgtest = xgb.DMatrix(test_x)\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "    cv = np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "        param = {'objective':'multi:softprob', 'seed':seed, 'num_class': 3, 'eval_metric':'mlogloss', \n",
    "                 'eta': 0.004, 'max_depth': 70, 'min_child_weight': 3,\n",
    "                 'colsample_bytree': 0.3, 'colsample_bylevel': 0.6, 'subsample': 0.8\n",
    "                }\n",
    "\n",
    "        xgbmodel = xgb.train(param, dtrain, 10000, watchlist, early_stopping_rounds=30, verbose_eval=None)\n",
    "        #joblib.dump(xgbmodel, f'./pred_pkl/XGB_{n+1}_fold_{seed}_seed_xgb.pkl')\n",
    "\n",
    "        cv[val_idx, :] = xgbmodel.predict(dvalid)\n",
    "        pred_test += xgbmodel.predict(xgtest) / 5\n",
    "        \n",
    "    pred_dict['xgb'+str(i+1)] = cv\n",
    "    pred_test_dict['xgb'+str(i+1)] = pred_test\n",
    "    print('multi_logloss:', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af31cf",
   "metadata": {},
   "source": [
    "xgbmodels_path = os.listdir('./pred_pkl/')\n",
    "xgbmodels_list = [x for x in xgbmodels_path if x.endswith(\"xgb.pkl\")]\n",
    "assert len(xgbmodels_list) == 15\n",
    "xgb_preds = np.zeros((test_x.shape[0], 3))\n",
    "xgtest = xgb.DMatrix(test_X)\n",
    "\n",
    "for m in xgbmodels_list:\n",
    "    xgbmodel = joblib.load('./pred_pkl/'+m)\n",
    "    xgb_preds_proba = xgbmodel.predict_proba(xgtest)\n",
    "    xgb_preds += xgb_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50eed7",
   "metadata": {},
   "source": [
    "## (3) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb673b7",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691af11e",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "params = {'max_depth': [55, 60, 65] # 튜닝할 파라미터 삽입\n",
    "            }\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state = 0, n_estimators = 1000, \n",
    "                                min_samples_leaf=2, min_samples_split=2,\n",
    "                                criterion='entropy', n_jobs = -1)\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid = params, cv = 5, n_jobs = -1)\n",
    "grid_cv.fit(df_train, y)\n",
    "\n",
    "print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d23e5",
   "metadata": {},
   "source": [
    "### 3 seeds, 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae779836",
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[42,2019,91373]\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "    cv = np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        rfmodel = RandomForestClassifier(n_estimators=1000, criterion='entropy', max_depth=55,\n",
    "                                         min_samples_leaf=2, min_samples_split=2,\n",
    "                                         random_state=seed)\n",
    "        rfmodel.fit(x_train, y_train)\n",
    "        #joblib.dump(rfmodel, f'./pred_pkl/RF_{n+1}_fold_{seed}_seed_rf.pkl')\n",
    "        \n",
    "        cv[val_idx, :] = rfmodel.predict_proba(x_val)        \n",
    "        pred_test += rfmodel.predict_proba(test_x) / 5\n",
    "        \n",
    "    pred_dict['rf'+str(i+1)] = cv\n",
    "    pred_test_dict['rf'+str(i+1)] = pred_test\n",
    "    print('multi_logloss :', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d0e02",
   "metadata": {},
   "source": [
    "rfmodels_path = os.listdir('./pred_pkl/')\n",
    "rfmodels_list = [x for x in rfmodels_path if x.endswith(\"rf.pkl\")]\n",
    "assert len(rfmodels_list) == 15\n",
    "rf_preds = np.zeros((test_x.shape[0], 3))\n",
    "\n",
    "for m in rfmodels_list:\n",
    "    rfmodel = joblib.load('./pred_pkl/'+m)\n",
    "    rf_preds_proba = rfmodel.predict_proba(test_x)\n",
    "    rf_preds += rf_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b2f34",
   "metadata": {},
   "source": [
    "## (4) Catboost (성능X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91681fa",
   "metadata": {},
   "source": [
    "lucky_seeds=[42,2019,91373]\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "    cv = np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        _train = Pool(x_train, label=y_train)\n",
    "        _valid = Pool(x_val, label=y_val)\n",
    "\n",
    "        catmodel =  CatBoostClassifier(loss_function='MultiClass', early_stopping_rounds=50, \n",
    "                                       random_state=seed, learning_rate=0.02, iterations=100000\n",
    "                                       #task_type=\"GPU\"\n",
    "                                      )\n",
    "        \n",
    "        catmodel.fit(_train, eval_set=_valid, use_best_model=True, verbose=2000)\n",
    "        #joblib.dump(rfmodel, f'./pred_pkl/RF_{n+1}_fold_{seed}_seed_rf.pkl')\n",
    "        \n",
    "        cv[val_idx, :] = catmodel.predict_proba(x_val)        \n",
    "        pred_test += catmodel.predict_proba(test_x) / 5\n",
    "        \n",
    "    pred_dict['cat'+str(i+1)] = cv\n",
    "    pred_test_dict['cat'+str(i+1)] = pred_test\n",
    "    print('multi_logloss :', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716bd37",
   "metadata": {},
   "source": [
    "## (4) Stacking (AutoLGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df3919",
   "metadata": {},
   "source": [
    "### 27features = 3seed(42, 2019, 91373) x 3model(lgb, xgb, rf) x 3class(0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c9d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(np.hstack([x for _, x in pred_dict.items()]))\n",
    "X_test = pd.DataFrame(np.hstack([x for _, x in pred_test_dict.items()]))\n",
    "\n",
    "pred = np.zeros((X_train.shape[0], 3), dtype=float)\n",
    "pred_test = np.zeros((X_test.shape[0], 3), dtype=float)\n",
    "#kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for i_cv, (i_trn, i_val) in enumerate(cv.split(X_train, train_y)):\n",
    "    if i_cv == 0:\n",
    "        clf = AutoLGB(objective='multiclass', metric='multi_logloss', params={'num_class': 3}, \n",
    "                      feature_selection=False, n_est=10000)\n",
    "        clf.tune(X_train.iloc[i_trn], train_y[i_trn])\n",
    "        n_best = clf.n_best\n",
    "        features = clf.features\n",
    "        params = clf.params\n",
    "        print(f'best iteration: {n_best}')\n",
    "        print(f'selected features ({len(features)}): {features}')        \n",
    "        print(params)\n",
    "        clf.fit(X_train.iloc[i_trn], train_y[i_trn])\n",
    "    else:\n",
    "        train_data = lgb.Dataset(X_train[features].iloc[i_trn], label=train_y[i_trn])\n",
    "        clf = lgb.train(params, train_data, n_best, verbose_eval=100)\n",
    "    \n",
    "    pred[i_val] = clf.predict(X_train[features].iloc[i_val])\n",
    "    pred_test += clf.predict(X_test[features]) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e6daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'CV Log Loss: {log_loss(train_y, pred):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124482db",
   "metadata": {},
   "source": [
    "# 결과 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4264cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_submission.copy()\n",
    "submission.iloc[:, 1:] = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32cfc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
