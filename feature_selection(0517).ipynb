{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea10db43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T13:44:40.796392Z",
     "start_time": "2021-05-17T13:44:36.534464Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from kaggler.model import AutoLGB\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, log_loss\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d0329b",
   "metadata": {},
   "source": [
    "# 1. 문제 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ffdbb8",
   "metadata": {},
   "source": [
    "# 2. 데이터 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ce40cf",
   "metadata": {},
   "source": [
    "## (1) 데이콘 기본 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0b78dae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T13:44:42.333964Z",
     "start_time": "2021-05-17T13:44:42.236848Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv').drop(['index'], axis=1).fillna('NAN')\n",
    "test = pd.read_csv('data/test.csv').drop(['index'], axis=1).fillna('NAN')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507dea16",
   "metadata": {},
   "source": [
    "# 3. 탐색적 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03fa11f",
   "metadata": {},
   "source": [
    "# 4. 변수 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4119fd6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T13:44:45.222089Z",
     "start_time": "2021-05-17T13:44:43.972942Z"
    }
   },
   "outputs": [],
   "source": [
    "# train데이터와 test데이터 변수를 함께 조정하기 위해 병합\n",
    "merge_data = pd.concat([train, test], axis = 0)\n",
    "\n",
    "# DAYS_BIRTH\n",
    "merge_data['DAYS_BIRTH_month']=np.floor((-merge_data['DAYS_BIRTH'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_BIRTH_week']=np.floor((-merge_data['DAYS_BIRTH'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_EMPLOYED\n",
    "merge_data['DAYS_EMPLOYED_month']=np.floor((-merge_data['DAYS_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['DAYS_EMPLOYED_week']=np.floor((-merge_data['DAYS_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# before_EMPLOYED\n",
    "merge_data['before_EMPLOYED']=merge_data['DAYS_BIRTH']-merge_data['DAYS_EMPLOYED']\n",
    "merge_data['before_EMPLOYED_month']=np.floor((-merge_data['before_EMPLOYED'])/30)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "merge_data['before_EMPLOYED_week']=np.floor((-merge_data['before_EMPLOYED'])/7)-(\n",
    "    (np.floor((-merge_data['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "# DAYS_BIRTH\n",
    "merge_data['1new_1'] = merge_data['DAYS_BIRTH_month'] / merge_data['income_total']\n",
    "merge_data['2new_1'] = merge_data['DAYS_BIRTH_week'] / merge_data['income_total']\n",
    "\n",
    "# DAYS_EMPLOYED\n",
    "merge_data['10new_1'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['11new_1'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# before_EMPLOYED\n",
    "merge_data['12new_1'] = merge_data['before_EMPLOYED'] / merge_data['income_total']\n",
    "merge_data['13new_1'] = merge_data['before_EMPLOYED_month'] / merge_data['income_total']\n",
    "merge_data['14new_1'] = merge_data['before_EMPLOYED_week'] / merge_data['income_total']\n",
    "\n",
    "# 총 수익을 가족 수로 나누기\n",
    "merge_data['15new_1'] = merge_data['income_total'] / merge_data['family_size']\n",
    "\n",
    "# 융합 삭제\n",
    "#merge_data['3new_1'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['DAYS_BIRTH_month']\n",
    "#merge_data['4new_1'] = merge_data['DAYS_EMPLOYED_month'] / merge_data['DAYS_BIRTH_week']\n",
    "#merge_data['5new_1'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['DAYS_BIRTH_month']\n",
    "#merge_data['6new_1'] = merge_data['DAYS_EMPLOYED_week'] / merge_data['DAYS_BIRTH_week']\n",
    "\n",
    "#merge_data['7new_1'] =  merge_data['begin_month'] / merge_data['DAYS_BIRTH_month']\n",
    "#merge_data['8new_1'] =  merge_data['begin_month'] / merge_data['DAYS_EMPLOYED_month']\n",
    "#merge_data['9new_1'] =  merge_data['begin_month'] / merge_data['before_EMPLOYED_month']\n",
    "\n",
    "merge_data['new_1'] = merge_data['child_num'] / merge_data['income_total']\n",
    "merge_data['new_2'] = merge_data['family_size'] / merge_data['income_total']\n",
    "merge_data['new_3'] = merge_data['DAYS_BIRTH'] / merge_data['income_total']\n",
    "merge_data['new_4'] = merge_data['DAYS_EMPLOYED'] / merge_data['income_total']\n",
    "#merge_data['new_5'] = merge_data['begin_month'] / merge_data['income_total']\n",
    "merge_data['new_6'] =  merge_data['DAYS_EMPLOYED'] / merge_data['DAYS_BIRTH']\n",
    "\n",
    "# 소득 skewed-data 처리\n",
    "merge_data['log1p_income_total'] = np.log1p(merge_data['income_total'])\n",
    "#merge_data['log_income_total'] = np.log(merge_data['income_total'])\n",
    "#merge_data['sqrt_income_total'] = np.sqrt(merge_data['income_total'])\n",
    "#merge_data['boxcox_income_total'] = stats.boxcox(merge_data['income_total'])[0]\n",
    "\n",
    "merge_data = merge_data.fillna(-999)\n",
    "train = merge_data[merge_data['credit'] != -999]\n",
    "test = merge_data[merge_data['credit'] == -999]\n",
    "test.drop('credit', axis = 1, inplace = True)\n",
    "\n",
    "train_cols = list(train.columns); train_cols.remove('credit'); train_cols.append('credit')\n",
    "train = train[train_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521fb435",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "train = train[train['child_num']<=6].reset_index(drop=True) # 아이의 수가 7명 이상인 데이터 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd3e25",
   "metadata": {},
   "source": [
    "## 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f90d4106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T13:44:49.313846Z",
     "start_time": "2021-05-17T13:44:49.288913Z"
    }
   },
   "outputs": [],
   "source": [
    "train_oh = train.copy()\n",
    "train_noh = train.copy()\n",
    "test_oh = test.copy()\n",
    "test_noh = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "632d6c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T13:44:50.385169Z",
     "start_time": "2021-05-17T13:44:50.353688Z"
    }
   },
   "outputs": [],
   "source": [
    "object_col = []\n",
    "for col in train_noh.columns:\n",
    "    if train_noh[col].dtype == 'object':\n",
    "        train_noh[col] = train_noh[col].astype('category')\n",
    "        test_noh[col] = test_noh[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2d2b896",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T13:44:51.306753Z",
     "start_time": "2021-05-17T13:44:51.225011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gender', 'car', 'reality', 'income_type', 'edu_type', 'family_type', 'house_type', 'occyp_type']\n"
     ]
    }
   ],
   "source": [
    "object_col = []\n",
    "for col in train_oh.columns:\n",
    "    if train_oh[col].dtype == 'object':\n",
    "        object_col.append(col)\n",
    "print(object_col)        \n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train_oh.drop(object_col, axis=1, inplace=True)\n",
    "train_oh = pd.concat([train_oh, train_onehot_df], axis=1)    \n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test_oh.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test_oh.drop(object_col, axis=1, inplace=True)\n",
    "test_oh = pd.concat([test_oh, test_onehot_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ca1c5",
   "metadata": {},
   "source": [
    "## Feature 하나씩 빼면서 성능 체크"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e03872",
   "metadata": {},
   "source": [
    "변수 하나씩 제거하면서 성능 체크<br>\n",
    "제거하여 성능이 좋게 나온 것들은 리스트에 따로 저장해두기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "455351cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T13:44:53.128923Z",
     "start_time": "2021-05-17T13:44:53.116955Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x = train_noh.drop(['credit'], axis=1)\n",
    "train_y = train_noh['credit']\n",
    "test_x = test_noh.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e0b022b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T13:44:56.347628Z",
     "start_time": "2021-05-17T13:44:56.336658Z"
    }
   },
   "outputs": [],
   "source": [
    "train_columns = list(train.columns)\n",
    "train_columns.remove('credit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b098dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection = ['income_total','begin_month']\n",
    "\n",
    "# for i in train_columns:\n",
    "#     if i in feature_selection:\n",
    "#         pass\n",
    "#     else: feature_selection.append(i)\n",
    "# feature_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72247840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T13:48:06.976713Z",
     "start_time": "2021-05-17T13:47:59.307622Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " multi_logloss: 0.71282953909921\n"
     ]
    }
   ],
   "source": [
    "# 기본 스코어\n",
    "score = []\n",
    "feature_selection = ['income_total','begin_month',\n",
    "                     'family_size','house_type',\n",
    "                     'DAYS_BIRTH','DAYS_EMPLOYED','work_phone','occyp_type','DAYS_BIRTH_month','DAYS_EMPLOYED_month','DAYS_EMPLOYED_week'\n",
    "                    , 'before_EMPLOYED','1new_1','2new_1','log1p_income_total', 'gender','car','reality','edu_type','family_type','before_EMPLOYED_month'\n",
    "                    , '10new_1', '12new_1','13new_1','14new_1','new_1','child_num'\n",
    "                    ,'DAYS_BIRTH_week','new_2','income_type']\n",
    "\n",
    "train_x = train_noh.loc[:,feature_selection]    \n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "    x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "    y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "    lgbm = LGBMClassifier(n_estimators=1000, objective='multiclass')\n",
    "    lgbm.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None)\n",
    "    cv[val_idx] = lgbm.predict_proba(x_val)\n",
    "print(f' multi_logloss: {log_loss(train_y, cv)}')\n",
    "score.append(log_loss(train_y, cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70a00d96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T14:23:58.411590Z",
     "start_time": "2021-05-17T14:23:57.665283Z"
    }
   },
   "outputs": [],
   "source": [
    "input1 = ['income_total','begin_month',\n",
    "                     'family_size','house_type',\n",
    "                     'DAYS_BIRTH','DAYS_EMPLOYED','work_phone','occyp_type','DAYS_BIRTH_month','DAYS_EMPLOYED_month','DAYS_EMPLOYED_week'\n",
    "                    , 'before_EMPLOYED','1new_1','2new_1','log1p_income_total', 'gender','car','reality','edu_type','family_type','before_EMPLOYED_month'\n",
    "                    , '10new_1', '12new_1','13new_1','14new_1','new_1','child_num'\n",
    "                    ,'DAYS_BIRTH_week','new_2','income_type']\n",
    "train_x1 = train_noh.loc[:,input1]\n",
    "train_x1.to_csv('data/feature_selection/train_x1.csv', index=False)\n",
    "\n",
    "test_x1 = test_noh.loc[:, input1]\n",
    "test_x1.shape\n",
    "test_x1.to_csv('data/feature_selection/test_x1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abfd4f4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T13:49:20.660712Z",
     "start_time": "2021-05-17T13:48:07.762985Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAG_MOBIL multi_logloss: 0.71282953909921\n",
      "phone multi_logloss: 0.7121395429164792\n",
      "email multi_logloss: 0.7119200471102657\n",
      "before_EMPLOYED_week multi_logloss: 0.7100241437950773\n",
      "11new_1 multi_logloss: 0.7107256761136765\n",
      "15new_1 multi_logloss: 0.7118014387952728\n",
      "new_3 multi_logloss: 0.7119667075104508\n",
      "new_4 multi_logloss: 0.7103687272671587\n",
      "new_6 multi_logloss: 0.7087851189202885\n"
     ]
    }
   ],
   "source": [
    "for i in train_columns:\n",
    "    if i in feature_selection:\n",
    "        continue\n",
    "    else: feature_selection.append(i)\n",
    "        \n",
    "    train_x = train_noh.loc[:,feature_selection]    \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        lgbm = LGBMClassifier(n_estimators=1000, objective='multiclass')\n",
    "        lgbm.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=None)\n",
    "        cv[val_idx] = lgbm.predict_proba(x_val)\n",
    "    print(f'{i} multi_logloss: {log_loss(train_y, cv)}')\n",
    "    \n",
    "    if log_loss(train_y, cv) < score[-1]:\n",
    "        score.append(log_loss(train_y, cv))\n",
    "    else: \n",
    "        feature_selection.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "245e6403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['income_total',\n",
       " 'begin_month',\n",
       " 'family_size',\n",
       " 'house_type',\n",
       " 'DAYS_BIRTH',\n",
       " 'DAYS_EMPLOYED',\n",
       " 'work_phone',\n",
       " 'occyp_type',\n",
       " 'DAYS_BIRTH_month',\n",
       " 'DAYS_EMPLOYED_month',\n",
       " 'DAYS_EMPLOYED_week',\n",
       " 'before_EMPLOYED',\n",
       " '1new_1',\n",
       " '2new_1',\n",
       " 'log1p_income_total',\n",
       " 'gender',\n",
       " 'car',\n",
       " 'reality',\n",
       " 'edu_type',\n",
       " 'family_type',\n",
       " 'before_EMPLOYED_month',\n",
       " '10new_1',\n",
       " '12new_1',\n",
       " '13new_1',\n",
       " '14new_1',\n",
       " 'new_1',\n",
       " 'child_num',\n",
       " 'DAYS_BIRTH_week',\n",
       " 'new_2',\n",
       " 'income_type',\n",
       " 'phone',\n",
       " 'email',\n",
       " 'before_EMPLOYED_week',\n",
       " 'new_6']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14723d8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T14:24:00.258774Z",
     "start_time": "2021-05-17T14:23:59.416958Z"
    }
   },
   "outputs": [],
   "source": [
    "input2 = feature_selection\n",
    "train_x2 = train_noh.loc[:,input2]\n",
    "train_x2.to_csv('data/feature_selection/train_x2.csv', index=False)\n",
    "\n",
    "test_x2 = test_noh.loc[:, input2]\n",
    "test_x2.shape\n",
    "test_x2.to_csv('data/feature_selection/test_x2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcfa9f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.to_csv('data/feature_selection/train_y.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e09a33",
   "metadata": {},
   "source": [
    "## 다시 인코딩 (lgb돌릴땐 할필요 없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b84406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:21:33.922697Z",
     "start_time": "2021-05-16T14:21:33.909733Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_oh = train.copy()\n",
    "# train_noh = train.copy()\n",
    "# test_oh = test.copy()\n",
    "# test_noh = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a86f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:21:34.699235Z",
     "start_time": "2021-05-16T14:21:34.676317Z"
    }
   },
   "outputs": [],
   "source": [
    "# object_col = []\n",
    "# for col in train_noh.columns:\n",
    "#     if train_noh[col].dtype == 'object':\n",
    "#         train_noh[col] = train_noh[col].astype('category')\n",
    "#         test_noh[col] = test_noh[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa55e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:21:35.588036Z",
     "start_time": "2021-05-16T14:21:35.519188Z"
    }
   },
   "outputs": [],
   "source": [
    "# object_col = []\n",
    "# for col in train_oh.columns:\n",
    "#     if train_oh[col].dtype == 'object':\n",
    "#         object_col.append(col)\n",
    "# print(object_col)        \n",
    "# enc = OneHotEncoder()\n",
    "# enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "# train_onehot_df = pd.DataFrame(enc.transform(train_oh.loc[:,object_col]).toarray(), \n",
    "#              columns=enc.get_feature_names(object_col))\n",
    "# train_oh.drop(object_col, axis=1, inplace=True)\n",
    "# train_oh = pd.concat([train_oh, train_onehot_df], axis=1)    \n",
    "\n",
    "# test_onehot_df = pd.DataFrame(enc.transform(test_oh.loc[:,object_col]).toarray(), \n",
    "#              columns=enc.get_feature_names(object_col))\n",
    "# test_oh.drop(object_col, axis=1, inplace=True)\n",
    "# test_oh = pd.concat([test_oh, test_onehot_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b92e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x1 = pd.read_csv('data/feature_selection/train_x1.csv').fillna('NAN')\n",
    "test_x1 = pd.read_csv('data/feature_selection/test_x1.csv').fillna('NAN')\n",
    "train_x2 = pd.read_csv('data/feature_selection/train_x2.csv').fillna('NAN')\n",
    "test_x2 = pd.read_csv('data/feature_selection/test_x2.csv').fillna('NAN')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa2eda",
   "metadata": {},
   "source": [
    "# 6. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dabcaf8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T13:54:02.916324Z",
     "start_time": "2021-05-17T13:54:02.909344Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "pred_test_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f2f4a",
   "metadata": {},
   "source": [
    "## (1) Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ddcf0c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T12:07:28.351105Z",
     "start_time": "2021-05-17T12:07:28.333153Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_noh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b6f379acef51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train_x = train_noh.drop(['credit'], axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_noh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'credit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# test_x = test_noh.copy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_noh' is not defined"
     ]
    }
   ],
   "source": [
    "# train_x = train_noh.drop(['credit'], axis=1)\n",
    "# train_y = train_noh['credit']\n",
    "# test_x = test_noh.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ead1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x1.copy()\n",
    "test_x = test_x1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a32a27e",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f195270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T13:09:03.066039Z",
     "start_time": "2021-05-17T13:06:20.880994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params:                                 \n",
      "{'colsample_bytree': 0.4, 'learning_rate': 0.0045, 'max_depth': -1, 'min_child_weight': 2, 'n_estimators': 5000, 'num_class': 3, 'num_leaves': 1000, 'objective': 'multiclass', 'reg_alpha': 0.94, 'reg_lambda': 0.98, 'seed': 2000, 'subsample': 1}\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.814234               \n",
      "[200]\tvalid_0's multi_logloss: 0.770095               \n",
      "[300]\tvalid_0's multi_logloss: 0.742408               \n",
      "[400]\tvalid_0's multi_logloss: 0.722981               \n",
      "[500]\tvalid_0's multi_logloss: 0.710038               \n",
      "[600]\tvalid_0's multi_logloss: 0.701622               \n",
      "[700]\tvalid_0's multi_logloss: 0.695501               \n",
      "[800]\tvalid_0's multi_logloss: 0.691893               \n",
      "[900]\tvalid_0's multi_logloss: 0.689553               \n",
      "[1000]\tvalid_0's multi_logloss: 0.688487              \n",
      "Early stopping, best iteration is:                    \n",
      "[1052]\tvalid_0's multi_logloss: 0.688245\n",
      "fold1 multi_logloss: 0.688244561828028                \n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.817766               \n",
      "[200]\tvalid_0's multi_logloss: 0.775904               \n",
      "  0%|          | 0/12 [01:34<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "SEED=2000\n",
    "\n",
    "# Hyperopt의 metric함수를 StratifiedKFold(cv=5)로 구하기\n",
    "def score(params):\n",
    "    print(\"Training with params: \")\n",
    "    print(params)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, random_state = SEED, shuffle = True)\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        lgbmodel = LGBMClassifier(**params)\n",
    "\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=100) \n",
    "        cv[val_idx, :] = lgbmodel.predict_proba(x_val)\n",
    "        print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}')\n",
    "    print('multi_logloss:', log_loss(train_y, cv))\n",
    "    score = log_loss(train_y, cv)\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "# Hyperopt의 범위를 지정해주고 max_evals만큼 반복한 후 최적의 파라미터를 반환\n",
    "def optimize(random_state=SEED):\n",
    "    \n",
    "#     param = {'objective':'multi:softprob', 'seed':SEED, 'num_class': 3, 'eval_metric':'mlogloss', \n",
    "#          'eta': 0.01, 'min_child_weight': 3,\n",
    "#          'colsample_bytree': 0.3, 'colsample_bylevel': 0.6, 'subsample': 0.8\n",
    "#         }\n",
    "    space = {\n",
    "        #'learning_rate': hp.quniform('learning_rate', 0.004, 0.006, 0.001),\n",
    "        'learning_rate' : 0.0045,\n",
    "        #'num_leaves': scope.int(hp.quniform('num_leaves', 1000, 1200, 50)),\n",
    "        'num_leaves' : 1000,\n",
    "        #'min_child_weight': hp.quniform('min_child_weight', 1, 3, 1),\n",
    "        'min_child_weight' : 2,\n",
    "        #'subsample': hp.quniform('subsample', 0.8, 1, 0.05),\n",
    "        'subsample' : 1,\n",
    "        #'colsample_bytree': hp.quniform('colsample_bytree', 0.3, 0.7, 0.05),\n",
    "        'colsample_bytree' : 0.4,\n",
    "        'max_depth' : -1,\n",
    "        'n_estimators' : 5000,\n",
    "        'objective' : 'multiclass',\n",
    "        'num_class' : 3,\n",
    "        'seed': SEED,\n",
    "         'reg_alpha': 0.94, \n",
    "        'reg_lambda': 0.98\n",
    "    }\n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters\n",
    "    best = fmin(score, space, algo=tpe.suggest, \n",
    "                # trials=trials, \n",
    "                max_evals=12)\n",
    "    return best\n",
    "\n",
    "best_hyperparams = optimize()\n",
    "print(\"The best hyperparameters are: \", \"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f91c48",
   "metadata": {},
   "source": [
    "### 3 seeds x 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5d65f37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T13:57:00.022481Z",
     "start_time": "2021-05-17T13:56:59.980566Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income_total</th>\n",
       "      <th>begin_month</th>\n",
       "      <th>family_size</th>\n",
       "      <th>house_type</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>work_phone</th>\n",
       "      <th>occyp_type</th>\n",
       "      <th>DAYS_BIRTH_month</th>\n",
       "      <th>DAYS_EMPLOYED_month</th>\n",
       "      <th>DAYS_EMPLOYED_week</th>\n",
       "      <th>before_EMPLOYED</th>\n",
       "      <th>1new_1</th>\n",
       "      <th>2new_1</th>\n",
       "      <th>log1p_income_total</th>\n",
       "      <th>gender</th>\n",
       "      <th>car</th>\n",
       "      <th>reality</th>\n",
       "      <th>edu_type</th>\n",
       "      <th>family_type</th>\n",
       "      <th>before_EMPLOYED_month</th>\n",
       "      <th>10new_1</th>\n",
       "      <th>12new_1</th>\n",
       "      <th>13new_1</th>\n",
       "      <th>14new_1</th>\n",
       "      <th>new_1</th>\n",
       "      <th>child_num</th>\n",
       "      <th>DAYS_BIRTH_week</th>\n",
       "      <th>new_2</th>\n",
       "      <th>income_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112500.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-21990</td>\n",
       "      <td>365243</td>\n",
       "      <td>0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-387233</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>11.630717</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-3.442071</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>Pensioner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135000.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-18964</td>\n",
       "      <td>-8671</td>\n",
       "      <td>0</td>\n",
       "      <td>Core staff</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-10293</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>11.813037</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.076244</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>State servant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69372.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-15887</td>\n",
       "      <td>-217</td>\n",
       "      <td>1</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-15670</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>11.147253</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>-0.225884</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>Working</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112500.0</td>\n",
       "      <td>-41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-19270</td>\n",
       "      <td>-2531</td>\n",
       "      <td>1</td>\n",
       "      <td>Drivers</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-16739</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.630717</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.148791</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>Commercial associate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>225000.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-17822</td>\n",
       "      <td>-9385</td>\n",
       "      <td>1</td>\n",
       "      <td>Managers</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8437</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>12.323860</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.037498</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>State servant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>202500.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-18593</td>\n",
       "      <td>-5434</td>\n",
       "      <td>1</td>\n",
       "      <td>Accountants</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13159</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.218500</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Incomplete higher</td>\n",
       "      <td>Married</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.064983</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>Working</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>202500.0</td>\n",
       "      <td>-34.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-10886</td>\n",
       "      <td>-1315</td>\n",
       "      <td>1</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-9571</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>12.218500</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.047264</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>Working</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>292500.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-21016</td>\n",
       "      <td>-14018</td>\n",
       "      <td>0</td>\n",
       "      <td>Medicine staff</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6998</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>12.586223</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.023925</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>Working</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>-33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-16541</td>\n",
       "      <td>-1085</td>\n",
       "      <td>0</td>\n",
       "      <td>NAN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-15456</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>12.100718</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.085867</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>Commercial associate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>270000.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-9154</td>\n",
       "      <td>-187</td>\n",
       "      <td>0</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-8967</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>12.506181</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.033211</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>Working</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      income_total  begin_month  family_size         house_type  DAYS_BIRTH  \\\n",
       "0         112500.0        -60.0          2.0  House / apartment      -21990   \n",
       "1         135000.0        -36.0          2.0  House / apartment      -18964   \n",
       "2          69372.0        -40.0          2.0  House / apartment      -15887   \n",
       "3         112500.0        -41.0          2.0  House / apartment      -19270   \n",
       "4         225000.0         -8.0          2.0  House / apartment      -17822   \n",
       "...            ...          ...          ...                ...         ...   \n",
       "9995      202500.0        -19.0          2.0  House / apartment      -18593   \n",
       "9996      202500.0        -34.0          2.0  House / apartment      -10886   \n",
       "9997      292500.0        -55.0          2.0  House / apartment      -21016   \n",
       "9998      180000.0        -33.0          2.0  House / apartment      -16541   \n",
       "9999      270000.0        -11.0          2.0  House / apartment       -9154   \n",
       "\n",
       "      DAYS_EMPLOYED  work_phone      occyp_type  DAYS_BIRTH_month  \\\n",
       "0            365243           0             NAN               1.0   \n",
       "1             -8671           0      Core staff               8.0   \n",
       "2              -217           1        Laborers               1.0   \n",
       "3             -2531           1         Drivers               6.0   \n",
       "4             -9385           1        Managers               6.0   \n",
       "...             ...         ...             ...               ...   \n",
       "9995          -5434           1     Accountants               7.0   \n",
       "9996          -1315           1        Laborers               2.0   \n",
       "9997         -14018           0  Medicine staff               4.0   \n",
       "9998          -1085           0             NAN              11.0   \n",
       "9999           -187           0        Laborers               5.0   \n",
       "\n",
       "      DAYS_EMPLOYED_month  DAYS_EMPLOYED_week  before_EMPLOYED    1new_1  \\\n",
       "0                    -7.0                -2.0          -387233  0.000009   \n",
       "1                     1.0                 2.0           -10293  0.000059   \n",
       "2                     7.0                 3.0           -15670  0.000014   \n",
       "3                     0.0                 1.0           -16739  0.000053   \n",
       "4                     0.0                 0.0            -8437  0.000027   \n",
       "...                   ...                 ...              ...       ...   \n",
       "9995                  1.0                 0.0           -13159  0.000035   \n",
       "9996                  7.0                 3.0            -9571  0.000010   \n",
       "9997                 11.0                 2.0            -6998  0.000014   \n",
       "9998                  0.0                 3.0           -15456  0.000061   \n",
       "9999                  6.0                 2.0            -8967  0.000019   \n",
       "\n",
       "        2new_1  log1p_income_total gender car reality  \\\n",
       "0     0.000009           11.630717      M   Y       N   \n",
       "1     0.000007           11.813037      F   N       Y   \n",
       "2     0.000014           11.147253      F   N       Y   \n",
       "3     0.000000           11.630717      M   Y       N   \n",
       "4     0.000009           12.323860      F   Y       Y   \n",
       "...        ...                 ...    ...  ..     ...   \n",
       "9995  0.000000           12.218500      F   Y       Y   \n",
       "9996  0.000015           12.218500      M   Y       Y   \n",
       "9997  0.000007           12.586223      F   N       Y   \n",
       "9998  0.000017           12.100718      F   Y       N   \n",
       "9999  0.000011           12.506181      F   N       Y   \n",
       "\n",
       "                           edu_type     family_type  before_EMPLOYED_month  \\\n",
       "0     Secondary / secondary special  Civil marriage                    7.0   \n",
       "1                  Higher education         Married                    7.0   \n",
       "2     Secondary / secondary special         Married                    6.0   \n",
       "3     Secondary / secondary special         Married                    5.0   \n",
       "4                  Higher education         Married                    5.0   \n",
       "...                             ...             ...                    ...   \n",
       "9995              Incomplete higher         Married                    6.0   \n",
       "9996  Secondary / secondary special  Civil marriage                    7.0   \n",
       "9997  Secondary / secondary special         Married                    5.0   \n",
       "9998  Secondary / secondary special         Married                   11.0   \n",
       "9999               Higher education         Married                   10.0   \n",
       "\n",
       "       10new_1   12new_1   13new_1   14new_1  new_1  child_num  \\\n",
       "0    -0.000062 -3.442071  0.000062  0.000027    0.0          0   \n",
       "1     0.000007 -0.076244  0.000052  0.000015    0.0          0   \n",
       "2     0.000101 -0.225884  0.000086  0.000029    0.0          0   \n",
       "3     0.000000 -0.148791  0.000044  0.000027    0.0          0   \n",
       "4     0.000000 -0.037498  0.000022  0.000004    0.0          0   \n",
       "...        ...       ...       ...       ...    ...        ...   \n",
       "9995  0.000005 -0.064983  0.000030  0.000015    0.0          0   \n",
       "9996  0.000035 -0.047264  0.000035  0.000015    0.0          0   \n",
       "9997  0.000038 -0.023925  0.000017  0.000010    0.0          0   \n",
       "9998  0.000000 -0.085867  0.000061  0.000000    0.0          0   \n",
       "9999  0.000022 -0.033211  0.000037  0.000004    0.0          0   \n",
       "\n",
       "      DAYS_BIRTH_week     new_2           income_type  \n",
       "0                 1.0  0.000018             Pensioner  \n",
       "1                 1.0  0.000015         State servant  \n",
       "2                 1.0  0.000029               Working  \n",
       "3                 0.0  0.000018  Commercial associate  \n",
       "4                 2.0  0.000009         State servant  \n",
       "...               ...       ...                   ...  \n",
       "9995              0.0  0.000010               Working  \n",
       "9996              3.0  0.000010               Working  \n",
       "9997              2.0  0.000007               Working  \n",
       "9998              3.0  0.000011  Commercial associate  \n",
       "9999              3.0  0.000007               Working  \n",
       "\n",
       "[10000 rows x 30 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4bde72b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T14:19:25.284025Z",
     "start_time": "2021-05-17T13:57:34.703843Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.791807\n",
      "[200]\tvalid_0's multi_logloss: 0.741585\n",
      "[300]\tvalid_0's multi_logloss: 0.713539\n",
      "[400]\tvalid_0's multi_logloss: 0.6962\n",
      "[500]\tvalid_0's multi_logloss: 0.68591\n",
      "[600]\tvalid_0's multi_logloss: 0.679928\n",
      "[700]\tvalid_0's multi_logloss: 0.677246\n",
      "[800]\tvalid_0's multi_logloss: 0.676356\n",
      "Early stopping, best iteration is:\n",
      "[836]\tvalid_0's multi_logloss: 0.676213\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.793394\n",
      "[200]\tvalid_0's multi_logloss: 0.743415\n",
      "[300]\tvalid_0's multi_logloss: 0.713765\n",
      "[400]\tvalid_0's multi_logloss: 0.695178\n",
      "[500]\tvalid_0's multi_logloss: 0.684008\n",
      "[600]\tvalid_0's multi_logloss: 0.677174\n",
      "[700]\tvalid_0's multi_logloss: 0.673221\n",
      "[800]\tvalid_0's multi_logloss: 0.671414\n",
      "[900]\tvalid_0's multi_logloss: 0.671073\n",
      "Early stopping, best iteration is:\n",
      "[894]\tvalid_0's multi_logloss: 0.671042\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.795592\n",
      "[200]\tvalid_0's multi_logloss: 0.748269\n",
      "[300]\tvalid_0's multi_logloss: 0.720888\n",
      "[400]\tvalid_0's multi_logloss: 0.704634\n",
      "[500]\tvalid_0's multi_logloss: 0.695876\n",
      "[600]\tvalid_0's multi_logloss: 0.691816\n",
      "[700]\tvalid_0's multi_logloss: 0.68986\n",
      "Early stopping, best iteration is:\n",
      "[731]\tvalid_0's multi_logloss: 0.689693\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.797683\n",
      "[200]\tvalid_0's multi_logloss: 0.751671\n",
      "[300]\tvalid_0's multi_logloss: 0.725695\n",
      "[400]\tvalid_0's multi_logloss: 0.710016\n",
      "[500]\tvalid_0's multi_logloss: 0.701746\n",
      "[600]\tvalid_0's multi_logloss: 0.698108\n",
      "[700]\tvalid_0's multi_logloss: 0.696672\n",
      "Early stopping, best iteration is:\n",
      "[736]\tvalid_0's multi_logloss: 0.696525\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.796174\n",
      "[200]\tvalid_0's multi_logloss: 0.749565\n",
      "[300]\tvalid_0's multi_logloss: 0.723636\n",
      "[400]\tvalid_0's multi_logloss: 0.708578\n",
      "[500]\tvalid_0's multi_logloss: 0.700282\n",
      "[600]\tvalid_0's multi_logloss: 0.695691\n",
      "[700]\tvalid_0's multi_logloss: 0.693747\n",
      "Early stopping, best iteration is:\n",
      "[732]\tvalid_0's multi_logloss: 0.693457\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.797318\n",
      "[200]\tvalid_0's multi_logloss: 0.750111\n",
      "[300]\tvalid_0's multi_logloss: 0.724287\n",
      "[400]\tvalid_0's multi_logloss: 0.709364\n",
      "[500]\tvalid_0's multi_logloss: 0.701169\n",
      "[600]\tvalid_0's multi_logloss: 0.697375\n",
      "[700]\tvalid_0's multi_logloss: 0.696194\n",
      "Early stopping, best iteration is:\n",
      "[707]\tvalid_0's multi_logloss: 0.696132\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.793567\n",
      "[200]\tvalid_0's multi_logloss: 0.74387\n",
      "[300]\tvalid_0's multi_logloss: 0.715061\n",
      "[400]\tvalid_0's multi_logloss: 0.697782\n",
      "[500]\tvalid_0's multi_logloss: 0.687284\n",
      "[600]\tvalid_0's multi_logloss: 0.681273\n",
      "[700]\tvalid_0's multi_logloss: 0.678054\n",
      "[800]\tvalid_0's multi_logloss: 0.677373\n",
      "Early stopping, best iteration is:\n",
      "[809]\tvalid_0's multi_logloss: 0.677278\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.795432\n",
      "[200]\tvalid_0's multi_logloss: 0.749302\n",
      "[300]\tvalid_0's multi_logloss: 0.723923\n",
      "[400]\tvalid_0's multi_logloss: 0.709737\n",
      "[500]\tvalid_0's multi_logloss: 0.702317\n",
      "[600]\tvalid_0's multi_logloss: 0.699253\n",
      "[700]\tvalid_0's multi_logloss: 0.698514\n",
      "Early stopping, best iteration is:\n",
      "[690]\tvalid_0's multi_logloss: 0.698437\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.797871\n",
      "[200]\tvalid_0's multi_logloss: 0.751513\n",
      "[300]\tvalid_0's multi_logloss: 0.725984\n",
      "[400]\tvalid_0's multi_logloss: 0.710655\n",
      "[500]\tvalid_0's multi_logloss: 0.702393\n",
      "[600]\tvalid_0's multi_logloss: 0.698263\n",
      "[700]\tvalid_0's multi_logloss: 0.696227\n",
      "Early stopping, best iteration is:\n",
      "[706]\tvalid_0's multi_logloss: 0.696077\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.792128\n",
      "[200]\tvalid_0's multi_logloss: 0.742666\n",
      "[300]\tvalid_0's multi_logloss: 0.716458\n",
      "[400]\tvalid_0's multi_logloss: 0.700836\n",
      "[500]\tvalid_0's multi_logloss: 0.692865\n",
      "[600]\tvalid_0's multi_logloss: 0.689071\n",
      "[700]\tvalid_0's multi_logloss: 0.68814\n",
      "Early stopping, best iteration is:\n",
      "[738]\tvalid_0's multi_logloss: 0.687906\n",
      "multi_logloss : 0.6882752660855695\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.792521\n",
      "[200]\tvalid_0's multi_logloss: 0.744978\n",
      "[300]\tvalid_0's multi_logloss: 0.718193\n",
      "[400]\tvalid_0's multi_logloss: 0.702566\n",
      "[500]\tvalid_0's multi_logloss: 0.693124\n",
      "[600]\tvalid_0's multi_logloss: 0.688055\n",
      "[700]\tvalid_0's multi_logloss: 0.68586\n",
      "[800]\tvalid_0's multi_logloss: 0.685209\n",
      "Early stopping, best iteration is:\n",
      "[808]\tvalid_0's multi_logloss: 0.685203\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.793281\n",
      "[200]\tvalid_0's multi_logloss: 0.746062\n",
      "[300]\tvalid_0's multi_logloss: 0.718862\n",
      "[400]\tvalid_0's multi_logloss: 0.703251\n",
      "[500]\tvalid_0's multi_logloss: 0.694794\n",
      "[600]\tvalid_0's multi_logloss: 0.690555\n",
      "[700]\tvalid_0's multi_logloss: 0.68911\n",
      "Early stopping, best iteration is:\n",
      "[752]\tvalid_0's multi_logloss: 0.688937\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.795746\n",
      "[200]\tvalid_0's multi_logloss: 0.751141\n",
      "[300]\tvalid_0's multi_logloss: 0.726489\n",
      "[400]\tvalid_0's multi_logloss: 0.712776\n",
      "[500]\tvalid_0's multi_logloss: 0.705431\n",
      "[600]\tvalid_0's multi_logloss: 0.702384\n",
      "Early stopping, best iteration is:\n",
      "[660]\tvalid_0's multi_logloss: 0.701997\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.794177\n",
      "[200]\tvalid_0's multi_logloss: 0.748518\n",
      "[300]\tvalid_0's multi_logloss: 0.723364\n",
      "[400]\tvalid_0's multi_logloss: 0.709662\n",
      "[500]\tvalid_0's multi_logloss: 0.702136\n",
      "[600]\tvalid_0's multi_logloss: 0.698467\n",
      "[700]\tvalid_0's multi_logloss: 0.69774\n",
      "Early stopping, best iteration is:\n",
      "[681]\tvalid_0's multi_logloss: 0.697479\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.796312\n",
      "[200]\tvalid_0's multi_logloss: 0.750484\n",
      "[300]\tvalid_0's multi_logloss: 0.723967\n",
      "[400]\tvalid_0's multi_logloss: 0.709019\n",
      "[500]\tvalid_0's multi_logloss: 0.700151\n",
      "[600]\tvalid_0's multi_logloss: 0.695487\n",
      "[700]\tvalid_0's multi_logloss: 0.693857\n",
      "Early stopping, best iteration is:\n",
      "[762]\tvalid_0's multi_logloss: 0.693267\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.791344\n",
      "[200]\tvalid_0's multi_logloss: 0.743085\n",
      "[300]\tvalid_0's multi_logloss: 0.715678\n",
      "[400]\tvalid_0's multi_logloss: 0.700043\n",
      "[500]\tvalid_0's multi_logloss: 0.690966\n",
      "[600]\tvalid_0's multi_logloss: 0.685937\n",
      "[700]\tvalid_0's multi_logloss: 0.683828\n",
      "Early stopping, best iteration is:\n",
      "[759]\tvalid_0's multi_logloss: 0.683495\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.79358\n",
      "[200]\tvalid_0's multi_logloss: 0.747393\n",
      "[300]\tvalid_0's multi_logloss: 0.721791\n",
      "[400]\tvalid_0's multi_logloss: 0.707067\n",
      "[500]\tvalid_0's multi_logloss: 0.698361\n",
      "[600]\tvalid_0's multi_logloss: 0.693695\n",
      "[700]\tvalid_0's multi_logloss: 0.692176\n",
      "Early stopping, best iteration is:\n",
      "[751]\tvalid_0's multi_logloss: 0.691726\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.793015\n",
      "[200]\tvalid_0's multi_logloss: 0.74625\n",
      "[300]\tvalid_0's multi_logloss: 0.719336\n",
      "[400]\tvalid_0's multi_logloss: 0.703672\n",
      "[500]\tvalid_0's multi_logloss: 0.694215\n",
      "[600]\tvalid_0's multi_logloss: 0.689564\n",
      "[700]\tvalid_0's multi_logloss: 0.687504\n",
      "Early stopping, best iteration is:\n",
      "[767]\tvalid_0's multi_logloss: 0.687116\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.787324\n",
      "[200]\tvalid_0's multi_logloss: 0.735923\n",
      "[300]\tvalid_0's multi_logloss: 0.705759\n",
      "[400]\tvalid_0's multi_logloss: 0.688335\n",
      "[500]\tvalid_0's multi_logloss: 0.677324\n",
      "[600]\tvalid_0's multi_logloss: 0.671248\n",
      "[700]\tvalid_0's multi_logloss: 0.668607\n",
      "[800]\tvalid_0's multi_logloss: 0.667302\n",
      "Early stopping, best iteration is:\n",
      "[816]\tvalid_0's multi_logloss: 0.667205\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.78989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's multi_logloss: 0.740928\n",
      "[300]\tvalid_0's multi_logloss: 0.712435\n",
      "[400]\tvalid_0's multi_logloss: 0.695826\n",
      "[500]\tvalid_0's multi_logloss: 0.685905\n",
      "[600]\tvalid_0's multi_logloss: 0.680546\n",
      "[700]\tvalid_0's multi_logloss: 0.677872\n",
      "[800]\tvalid_0's multi_logloss: 0.67714\n",
      "Early stopping, best iteration is:\n",
      "[816]\tvalid_0's multi_logloss: 0.677085\n",
      "multi_logloss : 0.6873519871514427\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.795908\n",
      "[200]\tvalid_0's multi_logloss: 0.74973\n",
      "[300]\tvalid_0's multi_logloss: 0.723283\n",
      "[400]\tvalid_0's multi_logloss: 0.707757\n",
      "[500]\tvalid_0's multi_logloss: 0.699072\n",
      "[600]\tvalid_0's multi_logloss: 0.694494\n",
      "[700]\tvalid_0's multi_logloss: 0.692528\n",
      "Early stopping, best iteration is:\n",
      "[746]\tvalid_0's multi_logloss: 0.692107\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.79014\n",
      "[200]\tvalid_0's multi_logloss: 0.741489\n",
      "[300]\tvalid_0's multi_logloss: 0.714262\n",
      "[400]\tvalid_0's multi_logloss: 0.698497\n",
      "[500]\tvalid_0's multi_logloss: 0.689891\n",
      "[600]\tvalid_0's multi_logloss: 0.68488\n",
      "[700]\tvalid_0's multi_logloss: 0.682941\n",
      "Early stopping, best iteration is:\n",
      "[728]\tvalid_0's multi_logloss: 0.682583\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.789724\n",
      "[200]\tvalid_0's multi_logloss: 0.739758\n",
      "[300]\tvalid_0's multi_logloss: 0.710476\n",
      "[400]\tvalid_0's multi_logloss: 0.692055\n",
      "[500]\tvalid_0's multi_logloss: 0.681034\n",
      "[600]\tvalid_0's multi_logloss: 0.674229\n",
      "[700]\tvalid_0's multi_logloss: 0.670667\n",
      "[800]\tvalid_0's multi_logloss: 0.668655\n",
      "[900]\tvalid_0's multi_logloss: 0.667865\n",
      "Early stopping, best iteration is:\n",
      "[900]\tvalid_0's multi_logloss: 0.667865\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.788589\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-e3e3ebcded20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                                    n_jobs=-1, random_state=seed)\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mlgbmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m#joblib.dump(lgbmodel, f'./pred_pkl/LGB_{n+1}_fold_{seed}_seed_lgb.pkl')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    845\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m         super(LGBMClassifier, self).fit(X, _y, sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    848\u001b[0m                                         \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m                                         \u001b[0meval_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[0minit_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbooster_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m         self._Booster = train(params, train_set,\n\u001b[0m\u001b[0;32m    613\u001b[0m                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    250\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   2456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2457\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot update due to null objective function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2458\u001b[1;33m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[0;32m   2459\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2460\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lucky_seeds=[42,2019,91373] # Lucky seed 늘려가면서 하기\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # CV 늘려가면서 하기\n",
    "    cv=np.zeros((train_x1.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x1.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x1, train_y)):\n",
    "\n",
    "        x_train, x_val = train_x1.iloc[train_idx], train_x1.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        lgbmodel = LGBMClassifier(learning_rate=0.005, objective='multiclass', n_estimators=10000, num_leaves=1000, \n",
    "                                  max_depth=-1, min_child_weight=2, colsample_bytree=0.6, reg_alpha=0.94, reg_lambda=0.98,\n",
    "                                   n_jobs=-1, random_state=seed)\n",
    "\n",
    "        lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=100) \n",
    "        #joblib.dump(lgbmodel, f'./pred_pkl/LGB_{n+1}_fold_{seed}_seed_lgb.pkl')\n",
    "\n",
    "        # CROSS-VALIDATION , EVALUATE CV\n",
    "        cv[val_idx,:] = lgbmodel.predict_proba(x_val)\n",
    "        pred_test += lgbmodel.predict_proba(test_x1) / 10 # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "    pred_dict['lgb'+str(i+1)] = cv\n",
    "    pred_test_dict['lgb'+str(i+1)] = pred_test\n",
    "        \n",
    "    print('multi_logloss :', log_loss(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a0297",
   "metadata": {},
   "source": [
    "lgbmodels_path = os.listdir('./pred_pkl/')\n",
    "lgbmodels_list = [x for x in lgbmodels_path if x.endswith(\"lgb.pkl\")]\n",
    "assert len(lgbmodels_list) == 15\n",
    "lgb_preds = np.zeros((test_x.shape[0], 3))\n",
    "\n",
    "for m in lgbmodels_list:\n",
    "    lgbmodel = joblib.load('./pred_pkl/'+m)\n",
    "    lgb_preds_proba = lgbmodel.predict_proba(test)\n",
    "    lgb_preds += lgb_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad65c6a",
   "metadata": {},
   "source": [
    "## (2) XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40aae89",
   "metadata": {},
   "source": [
    "원핫인코딩된 feature로 만들어주기 **꼭 밑에 코드 실행하고 XGBoost랑 Randomforest 돌리기!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b872a3ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:23:34.456443Z",
     "start_time": "2021-05-16T14:23:34.423450Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x = train_oh.drop(['credit'], axis=1)\n",
    "train_y = train_oh['credit']\n",
    "test_x = test_oh.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359bd6b7",
   "metadata": {},
   "source": [
    "### Parameter Tuning (hyperopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a26dd7",
   "metadata": {},
   "source": [
    "이거 오래걸리므로 안해도됨, 그리고 이미 했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22dfa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T17:07:47.991165Z",
     "start_time": "2021-05-16T14:45:57.913381Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SEED=0\n",
    "\n",
    "# Hyperopt의 metric함수를 StratifiedKFold(cv=5)로 구하기\n",
    "def score(params):\n",
    "    print(\"Training with params: \")\n",
    "    print(params)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, random_state = SEED, shuffle = True)\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "        xgbmodel = xgb.train(params, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=100)\n",
    "        cv[val_idx, :] = xgbmodel.predict(dvalid)\n",
    "        print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}')\n",
    "    print('multi_logloss:', log_loss(train_y, cv))\n",
    "    score = log_loss(train_y, cv)\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "# Hyperopt의 범위를 지정해주고 max_evals만큼 반복한 후 최적의 파라미터를 반환\n",
    "def optimize(random_state=SEED):\n",
    "    \n",
    "\n",
    "    space = {\n",
    "        'eta': hp.quniform('eta', 0.003, 0.006, 0.001),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 3, 1),\n",
    "        'subsample': hp.quniform('subsample', 0.6, 0.8, 0.05),\n",
    "        'gamma': hp.quniform('gamma', 0.6, 0.8, 0.05),\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.3, 0.7, 0.05),\n",
    "        'colsample_bylevel': hp.quniform('colsample_bylevel', 0.3, 0.7, 0.05),\n",
    "        'max_depth' : 100,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'objective' : 'multi:softprob',\n",
    "        'num_class' : 3,\n",
    "        'seed': SEED,\n",
    "    }\n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters\n",
    "    best = fmin(score, space, algo=tpe.suggest, \n",
    "                # trials=trials, \n",
    "                max_evals=10)\n",
    "    return best\n",
    "\n",
    "best_hyperparams = optimize()\n",
    "print(\"The best hyperparameters are: \", \"\\n\")\n",
    "print(best_hyperparams)\n",
    "params = best_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736cc9e",
   "metadata": {},
   "source": [
    "### 3 seeds x 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14423ed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:26:28.637607Z",
     "start_time": "2021-05-16T14:25:49.848336Z"
    }
   },
   "outputs": [],
   "source": [
    "lucky_seeds=[42, 2019, 91373] # 늘려가면서\n",
    "xgtest = xgb.DMatrix(test_x)\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # 늘려가면서\n",
    "    cv = np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "\n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "        param = params\n",
    "#         {'colsample_bylevel': 0.5, 'colsample_bytree': 0.4, 'eta': 0.003, 'eval_metric': 'mlogloss', \n",
    "#          'gamma': 0.7, 'max_depth': 100, 'min_child_weight': 2.0, 'num_class': 3, \n",
    "#          'objective': 'multi:softprob', 'seed': 0, 'subsample': 0.7}\n",
    "        xgbmodel = xgb.train(param, dtrain, 100000, watchlist, early_stopping_rounds=30, verbose_eval=100)\n",
    "        #joblib.dump(xgbmodel, f'./pred_pkl/XGB_{n+1}_fold_{seed}_seed_xgb.pkl')\n",
    "\n",
    "        cv[val_idx, :] = xgbmodel.predict(dvalid)\n",
    "        print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}')\n",
    "        pred_test += xgbmodel.predict(xgtest) / 10 # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "        \n",
    "    pred_dict['xgb'+str(i+1)] = cv\n",
    "    pred_test_dict['xgb'+str(i+1)] = pred_test\n",
    "    print('multi_logloss:', log_loss(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda16a86",
   "metadata": {},
   "source": [
    "xgbmodels_path = os.listdir('./pred_pkl/')\n",
    "xgbmodels_list = [x for x in xgbmodels_path if x.endswith(\"xgb.pkl\")]\n",
    "assert len(xgbmodels_list) == 15\n",
    "xgb_preds = np.zeros((test_x.shape[0], 3))\n",
    "xgtest = xgb.DMatrix(test_X)\n",
    "\n",
    "for m in xgbmodels_list:\n",
    "    xgbmodel = joblib.load('./pred_pkl/'+m)\n",
    "    xgb_preds_proba = xgbmodel.predict_proba(xgtest\n",
    "       xgb_preds += xgb_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26cc3d",
   "metadata": {},
   "source": [
    "## (3) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dd56d8",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae407e0",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "params = {'max_depth': [55, 60, 65] # 튜닝할 파라미터 삽입\n",
    "            }\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state = 0, n_estimators = 1000, \n",
    "                                min_samples_leaf=2, min_samples_split=2,\n",
    "                                criterion='entropy', n_jobs = -1)\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid = params, cv = 5, n_jobs = -1)\n",
    "grid_cv.fit(df_train, y)\n",
    "\n",
    "print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9109f9cc",
   "metadata": {},
   "source": [
    "### 3 seeds, 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "lucky_seeds=[42,2019,91373,53,1] # 늘려가면서\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state = seed, shuffle = True) # 늘려가면서\n",
    "    cv = np.zeros((train_x.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        rfmodel = RandomForestClassifier(n_estimators=1200, criterion='entropy', max_depth=60,\n",
    "                                         min_samples_leaf=2, min_samples_split=2,\n",
    "                                         random_state=seed)\n",
    "        rfmodel.fit(x_train, y_train)\n",
    "        #joblib.dump(rfmodel, f'./pred_pkl/RF_{n+1}_fold_{seed}_seed_rf.pkl')\n",
    "        \n",
    "        cv[val_idx, :] = rfmodel.predict_proba(x_val)      \n",
    "        print(f'fold{n+1} multi_logloss: {log_loss(y_val, cv[val_idx, :])}')\n",
    "        pred_test += rfmodel.predict_proba(test_x) / 10 # CV 바꾸면 이 숫자도 똑같이 바꿔야함\n",
    "        \n",
    "    pred_dict['rf'+str(i+1)] = cv\n",
    "    pred_test_dict['rf'+str(i+1)] = pred_test\n",
    "    print('multi_logloss :', log_loss(train_y, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842649e5",
   "metadata": {},
   "source": [
    "rfmodels_path = os.listdir('./pred_pkl/')\n",
    "rfmodels_list = [x for x in rfmodels_path if x.endswith(\"rf.pkl\")]\n",
    "assert len(rfmodels_list) == 15\n",
    "rf_preds = np.zeros((test_x.shape[0], 3))\n",
    "\n",
    "for m in rfmodels_list:\n",
    "    rfmodel = joblib.load('./pred_pkl/'+m)\n",
    "    rf_preds_proba = rfmodel.predict_proba(test_x)\n",
    "    rf_preds += rf_preds_proba/15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b541e2",
   "metadata": {},
   "source": [
    "## (4) Catboost (성능X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade79430",
   "metadata": {},
   "source": [
    "lucky_seeds=[42,2019,91373]\n",
    "for i, seed in enumerate(lucky_seeds):\n",
    "\n",
    "    kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "    cv = np.zeros((train.shape[0], 3))\n",
    "    pred_test = np.zeros((test_x.shape[0], 3), dtype=float)\n",
    "    \n",
    "    for n, (train_idx, val_idx) in enumerate(kfold.split(train)):\n",
    "        \n",
    "        x_train, x_val = train_x.iloc[train_idx], train_x.iloc[val_idx]\n",
    "        y_train, y_val = train_y.iloc[train_idx], train_y.iloc[val_idx]\n",
    "        _train = Pool(x_train, label=y_train)\n",
    "        _valid = Pool(x_val, label=y_val)\n",
    "\n",
    "        catmodel =  CatBoostClassifier(loss_function='MultiClass', early_stopping_rounds=50, \n",
    "                                       random_state=seed, learning_rate=0.02, iterations=100000\n",
    "                                       #task_type=\"GPU\"\n",
    "                                      )\n",
    "        \n",
    "        catmodel.fit(_train, eval_set=_valid, use_best_model=True, verbose=2000)\n",
    "        #joblib.dump(rfmodel, f'./pred_pkl/RF_{n+1}_fold_{seed}_seed_rf.pkl')\n",
    "        \n",
    "        cv[val_idx, :] = catmodel.predict_proba(x_val)        \n",
    "        pred_test += catmodel.predict_proba(test_x) / 5\n",
    "        \n",
    "    pred_dict['cat'+str(i+1)] = cv\n",
    "    pred_test_dict['cat'+str(i+1)] = pred_test\n",
    "    print('multi_logloss :', log_loss(true, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bcce66",
   "metadata": {},
   "source": [
    "## (4) Stacking (AutoLGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e20321",
   "metadata": {},
   "source": [
    "### 27features = 3seed(42, 2019, 91373) x 3model(lgb, xgb, rf) x 3class(0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(np.hstack([x for _, x in pred_dict.items()]))\n",
    "X_test = pd.DataFrame(np.hstack([x for _, x in pred_test_dict.items()]))\n",
    "\n",
    "pred = np.zeros((X_train.shape[0], 3), dtype=float)\n",
    "pred_test = np.zeros((X_test.shape[0], 3), dtype=float)\n",
    "#kfold = KFold(n_splits=5, random_state = seed, shuffle = True)\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42) # 이건 CV 너무 크게하면 안됨, 3~6까지 테스트해보면 좋을듯\n",
    "\n",
    "for i_cv, (i_trn, i_val) in enumerate(cv.split(X_train, train_y)):\n",
    "    if i_cv == 0:\n",
    "        clf = AutoLGB(objective='multiclass', metric='multi_logloss', params={'num_class': 3}, \n",
    "                      feature_selection=False, n_est=10000)\n",
    "        clf.tune(X_train.iloc[i_trn], train_y[i_trn])\n",
    "        n_best = clf.n_best\n",
    "        features = clf.features\n",
    "        params = clf.params\n",
    "        print(f'best iteration: {n_best}')\n",
    "        print(f'selected features ({len(features)}): {features}')        \n",
    "        print(params)\n",
    "        clf.fit(X_train.iloc[i_trn], train_y[i_trn])\n",
    "    else:\n",
    "        train_data = lgb.Dataset(X_train[features].iloc[i_trn], label=train_y[i_trn])\n",
    "        clf = lgb.train(params, train_data, n_best, verbose_eval=100)\n",
    "    \n",
    "    pred[i_val] = clf.predict(X_train[features].iloc[i_val])\n",
    "    pred_test += clf.predict(X_test[features]) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1eb2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T14:30:21.694306Z",
     "start_time": "2021-05-16T14:30:21.688317Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442235be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'CV Log Loss: {log_loss(train_y, pred):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa37556",
   "metadata": {},
   "source": [
    "# 결과 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_submission.copy()\n",
    "submission.iloc[:, 1:] = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e10ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
